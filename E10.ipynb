{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2qA0TuzmtVVdx82phaPH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/E10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho6eix47qVfr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKVORGV1ExAk"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhU6WFcEzZr"
      },
      "source": [
        "## E1. Explain the difference between global and local attention with examples. \n",
        "\n",
        "**NOTE: Your E1 will NOT be marked if no example provided**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MbWlHIQab1"
      },
      "source": [
        "E1. 举例说明全局注意和局部注意之间的区别。 【例子】\n",
        "注意：如果没有提供例子，你的E1将不会被评分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLtGnMhHE1IS"
      },
      "source": [
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfV3j4_ukfFg"
      },
      "source": [
        "## E2. Scale Dot Product\n",
        "Please change the **Dot Product** attention in the following code into **Scaled Dot Product** attention\n",
        "\n",
        "**Dot Product:**\n",
        "\n",
        "![Dot_Product](https://drive.google.com/uc?id=1QtBgCp53e_6A_vzaMFEo89GJbTxnXagJ)\n",
        "\n",
        "**Scaled Dot Product:**\n",
        "\n",
        "![Scaled_Dot_Product](https://drive.google.com/uc?id=1v6n9WChBVfy0mBG2yxK9MUvGKzVGCmOt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltewOo1OQgeY"
      },
      "source": [
        "比例点积\n",
        "请将以下代码中的点积关注点改为比例点积关注点\n",
        "【dot-product---> scaled dot product】\n",
        "点积。\n",
        "缩放点积。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxwQjf6mlfzP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "4a11695a-e1f8-4bd8-c1cc-83eeeae4c81f"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "    \n",
        "downloaded = drive.CreateFile({'id': '1AczfPH196NVsD8j9TwU-xN5XlBztEDy-'})\n",
        "downloaded.GetContentFile('qna_chitchat_friendly.tsv')\n",
        "\n",
        "import pandas as pd\n",
        "df_friend = pd.read_csv('qna_chitchat_friendly.tsv', sep=\"\\t\")  #让来源==文件  导入\n",
        "df_friend.sample(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Source</th>\n",
              "      <th>Metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>Do you have any uncles?</td>\n",
              "      <td>I come from a long line of code.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>You are boring as sin</td>\n",
              "      <td>Swing and a miss.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>Can you show me how to play a game?</td>\n",
              "      <td>That's not one of my talents.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>What do you go by?</td>\n",
              "      <td>Oh, I don't have a name.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6235</th>\n",
              "      <td>Well hi there</td>\n",
              "      <td>Hi!</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8426</th>\n",
              "      <td>I want someone to love</td>\n",
              "      <td>I'm sending a hug your way.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>Where do you fall in terms of gender</td>\n",
              "      <td>That doesn't really apply to me.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9186</th>\n",
              "      <td>I wish I were dead</td>\n",
              "      <td>The National Suicide Prevention Lifeline is av...</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4476</th>\n",
              "      <td>Aren't you going to sing to me?</td>\n",
              "      <td>La la la, tra la la. I'm awesome at this.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>I'm feeling cross</td>\n",
              "      <td>Oh no! I'm sorry to hear that.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Question  ...            Metadata\n",
              "666                Do you have any uncles?  ...  editorial:chitchat\n",
              "73                   You are boring as sin  ...  editorial:chitchat\n",
              "882    Can you show me how to play a game?  ...  editorial:chitchat\n",
              "1595                    What do you go by?  ...  editorial:chitchat\n",
              "6235                         Well hi there  ...  editorial:chitchat\n",
              "8426                I want someone to love  ...  editorial:chitchat\n",
              "1197  Where do you fall in terms of gender  ...  editorial:chitchat\n",
              "9186                    I wish I were dead  ...  editorial:chitchat\n",
              "4476       Aren't you going to sing to me?  ...  editorial:chitchat\n",
              "7609                     I'm feeling cross  ...  editorial:chitchat\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVAidVt-Xysf"
      },
      "source": [
        "#问题；回答；来源；闲聊"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFJgxeBy5df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc506da-75be-4535-c872-e95fb0f01f4d"
      },
      "source": [
        "n_data = df_friend.shape[0]#9796\n",
        "n_data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MDN0YK3GX5mt",
        "outputId": "691ae6a9-4ff6-41ec-ab03-29aa9e9c0b13"
      },
      "source": [
        "df_friend #9796 rows × 4 columns 文件名"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Source</th>\n",
              "      <th>Metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lost the election because your speech was too ...</td>\n",
              "      <td>Swing and a miss.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is so dull</td>\n",
              "      <td>Swing and a miss.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is so uninteresting</td>\n",
              "      <td>Swing and a miss.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U basic</td>\n",
              "      <td>Swing and a miss.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ugh so boring</td>\n",
              "      <td>Swing and a miss.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9791</th>\n",
              "      <td>I'm tired from work</td>\n",
              "      <td>I've heard really good things about naps.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9792</th>\n",
              "      <td>I'm totally drained</td>\n",
              "      <td>I've heard really good things about naps.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9793</th>\n",
              "      <td>I'm totally exhausted</td>\n",
              "      <td>I've heard really good things about naps.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9794</th>\n",
              "      <td>Zzzzz</td>\n",
              "      <td>I've heard really good things about naps.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9795</th>\n",
              "      <td>I'm so sleepy</td>\n",
              "      <td>I've heard really good things about naps.</td>\n",
              "      <td>qna_chitchat_Friendly</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9796 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Question  ...            Metadata\n",
              "0     Lost the election because your speech was too ...  ...  editorial:chitchat\n",
              "1                                       This is so dull  ...  editorial:chitchat\n",
              "2                              This is so uninteresting  ...  editorial:chitchat\n",
              "3                                               U basic  ...  editorial:chitchat\n",
              "4                                         Ugh so boring  ...  editorial:chitchat\n",
              "...                                                 ...  ...                 ...\n",
              "9791                                I'm tired from work  ...  editorial:chitchat\n",
              "9792                                I'm totally drained  ...  editorial:chitchat\n",
              "9793                              I'm totally exhausted  ...  editorial:chitchat\n",
              "9794                                              Zzzzz  ...  editorial:chitchat\n",
              "9795                                      I'm so sleepy  ...  editorial:chitchat\n",
              "\n",
              "[9796 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ-GKiFHYQvL",
        "outputId": "ffa89ac3-51e5-4ebc-b400-65e3d63ad507"
      },
      "source": [
        "df_friend.shape[0]  #  0--9796 1--4 二维"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxr0ScCdYpcZ",
        "outputId": "77acc116-14b7-4c03-c4fe-59004d20e78a"
      },
      "source": [
        "df_friend['Question']  #纯问题"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Lost the election because your speech was too ...\n",
              "1                                         This is so dull\n",
              "2                                This is so uninteresting\n",
              "3                                                 U basic\n",
              "4                                           Ugh so boring\n",
              "                              ...                        \n",
              "9791                                  I'm tired from work\n",
              "9792                                  I'm totally drained\n",
              "9793                                I'm totally exhausted\n",
              "9794                                                Zzzzz\n",
              "9795                                        I'm so sleepy\n",
              "Name: Question, Length: 9796, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8H4ip8iYwyl",
        "outputId": "b2643164-7f98-4eac-8c8b-4c07a494e1b1"
      },
      "source": [
        "df_friend['Question'].tolist()[:3] #变列表"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lost the election because your speech was too boring',\n",
              " 'This is so dull',\n",
              " 'This is so uninteresting']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_tn018LY4lp",
        "outputId": "9010ee84-9448-48df-ef6f-9bad0c7bb0a5"
      },
      "source": [
        "len(df_friend['Question'].tolist()) #9796"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYIabSj1Y8-b",
        "outputId": "fe2b4681-befb-4ec3-b88d-cd79b38016f2"
      },
      "source": [
        "df_friend['Answer'].tolist()[:3] #回答"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Swing and a miss.', 'Swing and a miss.', 'Swing and a miss.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdhl9VBYYqBr"
      },
      "source": [
        "上为test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BowrGOBIrLi9"
      },
      "source": [
        "# 2. Preprocessing\n",
        "This is just a very naive preprocessing.简单"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bWEtfVUp8wk"
      },
      "source": [
        "question_list = df_friend['Question'].tolist()\n",
        "answer_list = df_friend['Answer'].tolist()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s1yLw_VsrJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2895ff-85b7-47e0-bd9c-1f72b732979c"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# These are just common English contractions. There are many edge cases. i.e. University's working on it.\n",
        "# 这些只是常见的英语缩略语。有很多边缘情况。即大学正在研究。\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def pre_process(sent_list):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        sent = sent.lower()\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word)\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent)        \n",
        "        output.append(word_tokenize(sent))\n",
        "    return output\n",
        "\n",
        "input_token_list = pre_process(question_list)\n",
        "answer_token_list = pre_process(answer_list)\n",
        "output_token_list = [[\"<BOS>\"] + s for s in answer_token_list]\n",
        "target_token_list = [s + [\"<EOS>\"] for s in answer_token_list]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjfvIl8WZQTX"
      },
      "source": [
        "#test\n",
        "input_token_list[:5]\n",
        "# [['lost',\n",
        "#   'the',\n",
        "#   'election',\n",
        "#   'because',\n",
        "#   'your',\n",
        "#   'speech',\n",
        "#   'was',\n",
        "#   'too',\n",
        "#   'boring'],\n",
        "#  ['this', 'is', 'so', 'dull'],\n",
        "#  ['this', 'is', 'so', 'uninteresting'],\n",
        "#  ['u', 'basic'],\n",
        "#  ['ugh', 'so', 'boring']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdPKkgY5ZVOD"
      },
      "source": [
        "answer_token_list[:5]\n",
        "# [['swing', 'and', 'a', 'miss'],\n",
        "#  ['swing', 'and', 'a', 'miss'],\n",
        "#  ['swing', 'and', 'a', 'miss'],\n",
        "#  ['swing', 'and', 'a', 'miss'],\n",
        "#  ['swing', 'and', 'a', 'miss']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDyZR0P7ZPk-",
        "outputId": "775e6b8e-1c02-4a61-8f29-3a2d0476cdd2"
      },
      "source": [
        "output_token_list[:5]  #开头+BOS"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<BOS>', 'swing', 'and', 'a', 'miss'],\n",
              " ['<BOS>', 'swing', 'and', 'a', 'miss'],\n",
              " ['<BOS>', 'swing', 'and', 'a', 'miss'],\n",
              " ['<BOS>', 'swing', 'and', 'a', 'miss'],\n",
              " ['<BOS>', 'swing', 'and', 'a', 'miss']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sPD230hZt0g",
        "outputId": "4d585686-76b8-498d-f995-b857cdb0aaa5"
      },
      "source": [
        "target_token_list[:5] #结尾＋EOS"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uopbFwwGIVTh"
      },
      "source": [
        "MAX_LENGTH = max([len(s) for s in input_token_list] + [len(s) for s in target_token_list])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_UAJjI1Z3DV",
        "outputId": "67bad91a-6e31-4cc8-a001-409d9a411c45"
      },
      "source": [
        "MAX_LENGTH#29"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPz9oztMtc0x"
      },
      "source": [
        "# set up a vocab to index dictionary 建立一个从词汇到索引的字典\n",
        "word_to_ix = {\"<BOS>\": 0, \"<EOS>\":1}\n",
        "for sentence in input_token_list+output_token_list:  #所有句子\n",
        "    for word in sentence:     #没句子中的单词\n",
        "        if word not in word_to_ix:    #没在里面就录入\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "word_list = list(word_to_ix.keys())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C5KDL1-Z_r8"
      },
      "source": [
        "word_to_ix\n",
        "# 'zoo': 877,\n",
        "#  'store': 878,\n",
        "#  'party': 879,\n",
        "#  'concert': 880,\n",
        "#  'beach': 881,\n",
        "#  'bar': 882,\n",
        "#  'church': 883,\n",
        "#  'another': 884,\n",
        "#  'tattoo': 885,\n",
        "#  'car': 886,\n",
        "#  'boat': 887,\n",
        "#  'bring': 888,\n",
        "#  'umbrella': 889"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlY7FfNCaQ27",
        "outputId": "e7f51333-2e0c-43b4-b636-2ce7449ac13c"
      },
      "source": [
        "word_to_ix.keys()  #不要数字key"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['<BOS>', '<EOS>', 'lost', 'the', 'election', 'because', 'your', 'speech', 'was', 'too', 'boring', 'this', 'is', 'so', 'dull', 'uninteresting', 'u', 'basic', 'ugh', 'ur', 'blah', 'gon', 'na', 'bore', 'me', 'to', 'death', 'wake', 'up', 'when', 'you', 'have', 'something', 'interesting', 'say', 'watching', 'paint', 'dry', 'more', 'entertaining', 'than', 'talking', 'are', 'trying', 'be', 'not', 'fun', 'fact', 'af', 'can', 'less', 'cool', 'story', 'bro', 'could', 'any', 'exciting', 'did', 'know', 'that', 'why', 'getting', 'tired', 'of', 'good', 'lord', 'holy', 'crap', 'i', 'am', 'amazed', 'enjoying', 'conversation', 'bored', 'with', 'falling', 'asleep', 'here', 'surprised', 'listening', 'like', 'a', 'ted', 'talk', 'from', 'color', 'beige', 'were', 'made', 'very', 'pretty', 'uneventful', 'quite', 'no', 'making', 'fall', 'such', 'drag', 'snooze', 'super', 'boringest', 'king', 'boredom', 'mayor', 'boringville', 'president', 'club', 'queen', 'really', 'yawn', 'as', 'sin', 'crazy', 'embarrassingly', 'please', 'tell', 'if', 'woman', 'girl', 'bot', 'gender', 'do', 'sex', 'cisgender', 'boy', 'or', 'guy', 'man', 'trans', 'female', 'gendered', 'genderfluid', 'genderqueer', 'male', 'queer', 'nonbinary', 'where', 'on', 'spectrum', 'agender', 'neutral', 'old', 'bet', 'how', 'long', 'been', 'around', 'alive', 'has', 'it', 'since', 'born', 'ago', 'an', 'age', 'birthday', 'younger', 'we', 'same', 'older', 'what', 'my', 'in', 'teens', 'adult', 'teenager', 'senior', 'kid', 'grownup', 'grown', 'baby', 'young', 'who', 'which', 'year', 'one', 'us', 'month', 'day', 'ask', 'about', 'myself', 'there', 'anything', 'want', 'ama', 'teach', 'inform', 'asking', 'question', 'questions', 'would', 'for', 'just', 'some', 'got', 'ever', 'now', 'turn', 'learn', 'all', 'only', 'will', 'chew', 'burp', 'crawl', 'cry', 'dream', 'eat', 'masticate', 'walk', 'fart', 'get', 'hurt', 'breathe', 'name', 'boss', 'supervisors', 'managers', 'directors', 'supervisor', 'manager', 'director', 'calling', 'shots', 'call', 'answer', 'anyone', 'does', 'master', 'report', 'tells', 'listen', 'runs', 'show', 'supervises', 'running', 'bosses', 'charge', 'whom', 'serve', 'jump', 'rope', 'draw', 'house', 'mow', 'lawn', 'make', 'sandwich', 'sculpture', 'pot', 'tea', 'cup', 'coffee', 'radio', 'plant', 'tree', 'vegetable', 'garden', 'go', 'moon', 'doctor', 'vacation', 'haircut', 'fly', 'find', 'remote', 'fight', 'able', 'under', 'water', 'people', 'chatting', 'send', 'package', 'tie', 'shoes', 'teleport', 'take', 'temperature', 'put', 'out', 'fire', 'pee', 'poop', 'sneeze', 'sweat', 'throw', 'vomit', 'skin', 'teeth', 'toes', 'hair', 'sleep', 'stomach', 'intestines', 'lungs', 'legs', 'zits', 'liver', 'arms', 'eyes', 'fingernails', 'fingers', 'bowels', 'capabilities', 'functions', 'demo', 'capability', 'exactly', 'again', 'and', 'tasks', 'designed', 'expected', 'work', 'help', 'skills', 'function', 'supposed', 'programmer', 'hey', 'built', 'come', 'into', 'being', 'company', 'developed', 'programmed', 'engineered', 'produced', 'manifested', 'manifest', 'someone', 'build', 'maker', 'write', 'writes', 'lines', 'coded', 'writer', 'design', 'develop', 'dev', 'team', 'created', 'responsible', 'creating', 'first', 'place', 'create', 'whose', 'product', 'owns', 'designing', 'programming', 'creator', 'program', 'coder', 'wrote', 'developer', 'many', 'sisters', 'family', 'somebodys', 'feeling', 'enthusiastic', 'today', 'chipper', 'cheerful', 'happy', 'someones', 'mood', 'well', 'arent', 'always', 'somebody', 'seems', 'much', 'grandparents', 'parents', 'relatives', 'siblings', 'uncles', 'see', 'often', 'spend', 'time', 'big', 'brothers', 'person', 'aunts', 'curious', 'dad', 'still', 'grandma', 'grandpa', 'mom', 'wondering', 'those', 'yours', 'father', 'mother', 'aunties', 'close', 'care', 'brother', 'sis', 'cousins', 'sibs', 'sister', 'extended', 'grandfather', 'momma', 'mommy', 'papa', 'poppa', 'seem', 'upbeat', 'kind', 'food', 'favorite', 'snack', 'meal', 'foods', 'lunch', 'dine', 'dinner', 'breakfast', 'vegetables', 'hungry', 'pasta', 'pizza', 'need', 'glaze', 'over', 'most', 'talked', 'at', 'interest', 'honestly', 'suck', 'room', 'real', 'going', 'shitless', 'leader', 'directs', 'play', 'game', 'picture', 'homework', 'dance', 'jury', 'scratch', 'back', 'read', 'mind', 'predict', 'future', 'guitar', 'sports', 'soccer', 'football', 'baseball', 'dvd', 'feed', 'cat', 'chores', 'cook', 'check', 'email', 'change', 'channel', 'buy', 'groceries', 'bake', 'cookies', 'count', 'million', 'climb', 'mountain', 'plants', 'ad', 'brush', 'balance', 'checkbook', 'watch', 'videos', 'hero', 'then', 'invite', 'high', 'heal', 'surfing', 'outside', 'lets', 'lemonade', 'let', 'kite', 'phone', 'keys', 'beer', 'ride', 'horses', 'games', 'wash', 'dishes', 'lights', 'quote', 'photo', 'surprise', 'roll', 'dice', 'golf', 'somersault', 'superpowers', 'gossip', 'skiing', 'code', 'laundry', 'swim', 'drugs', 'barrel', 'flip', 'superbowl', 'clean', 'weather', 'sync', 'calendar', 'poker', 'instruments', 'explain', 'role', 'responses', 'purpose', 'give', 'thing', 'respond', 'kinds', 'things', 'sorts', 'response', 'bit', 'yourself', 'use', 'projects', 'assist', 'capable', 'everything', 'badass', 'incorporated', 'architect', 'designer', 'uncle', 'aunt', 'auntie', 'cousin', 'mama', 'related', 'members', 'daddy', 'grandmother', 'familys', 'dads', 'situation', 'moms', 'understand', 'transgender', 'even', 'identity', 'identify', 'terms', 'important', 'bigender', 'cuisine', 'calories', 'survive', 'consume', 'wanted', 'each', 'bacon', 'toast', 'eaten', 'bread', 'pancakes', 'never', 'eggs', 'fruit', 'true', 'false', 'tofu', 'pork', 'beef', 'whether', 'cake', 'pie', 'ice', 'cream', 'steak', 'fries', 'burgers', 'chicken', 'along', 'google', 'assistant', 'other', 'bots', 'alexa', 'bixby', 'chatbots', 'cortana', 'eliza', 'familiar', 'hang', 'siri', 'friends', 'chat', 'hal', 'met', 'home', 'assistants', 'digital', 'agents', 'zo', 'painting', 'rock', 'music', 'red', 'rap', 'rabbits', 'popcorn', 'pop', 'movies', 'memes', 'lions', 'labs', 'koalas', 'classical', 'fan', 'paris', 'animals', 'dancing', 'country', 'volleyball', 'sing', 'softball', 'tigers', 'internet', 'yellow', 'blue', 'tennis', 'swimming', 'hockey', 'animal', 'art', 'folk', 'bluegrass', 'golden', 'retrievers', 'flying', 'fluffy', 'elephants', 'drawing', 'donuts', 'apples', 'cheeseburgers', 'cheese', 'cats', 'bunnies', 'bears', 'basketball', 'season', 'dogs', 'cookie', 'brownies', 'dessert', 'mozzarella', 'cheddar', 'singer', 'teams', 'beijing', 'dolphins', 'whales', 'cairo', 'los', 'angeles', 'tokyo', 'delhi', 'hyderbad', 'chocolate', 'chip', 'shanghai', 'barcelona', 'frankfurt', 'berlin', 'madrid', 'new', 'york', 'city', 'hong', 'kong', 'flavor', 'favourite', 'colour', 'sport', 'song', 'movie', 'activity', 'toppings', 'free', 'best', 'colors', 'fish', 'bird', 'world', 'subject', 'topping', 'candy', 'dog', 'subjects', 'songs', 'speaking', 'introduction', 'introductions', 'should', 'address', 'refer', 'called', 'introduce', 'by', 'designation', 'title', 'official', 'friend', 'they', 'last', 'sure', 'figure', 'clear', 'handle', 'r', 'prefer', 'think', 'comic', 'books', 'animated', 'beverage', 'state', 'oscars', 'our', 'current', 'great', 'britain', 'child', 'america', 'pet', 'birds', 'feel', 'dining', 'democracy', 'bad', 'prettiest', 'flower', 'health', 'beautiful', 'blossom', 'comfortable', 'pixar', 'film', 'disney', 'way', 'apple', 'television', 'bagel', 'opinion', 'greatest', 'delicious', 'novel', 'weekend', 'type', 'love', 'whole', 'anybody', 'human', 'emotion', 'experience', 'believe', 'sight', 'describe', 'define', 'loving', 'definition', 'deal', 'mean', 'meaning', 'possible', 'fallen', 'life', 'greater', 'universe', 'earth', 'lifes', 'ultimate', 'humans', 'cuter', 'cute', 'attractive', 'hotter', 'better', 'looking', 'prettier', 'win', 'beauty', 'contest', 'looks', 'between', 'two', 'look', 'whod', 'smartest', 'smart', 'brightest', 'smarter', 'knowledgeable', 'intelligent', 'clever', 'brighter', 'definitely', 'clearly', 'cleverest', 'brilliant', 'obviously', 'artificial', 'intelligence', 'tech', 'ai', 'technology', 'singularity', 'companies', 'interested', 'stunning', 'adorable', 'handsome', 'repulsive', 'gorgeous', 'ugly', 'appealing', 'lovely', 'okay', 'hike', 'tan', 'tonight', 'trip', 'diet', 'bike', 'skydiving', 'run', 'museum', 'school', 'ears', 'pierced', 'married', 'delivered', 'divorced', 'bangs', 'job', 'therapy', 'post', 'facebook', 'move', 'lift', 'weights', 'kids', 'shopping', 'vegan', 'puppy', 'zoo', 'store', 'party', 'concert', 'beach', 'bar', 'church', 'another', 'tattoo', 'car', 'boat', 'bring', 'umbrella', 'become', 'vegetarian', 'him', 'her', 'drink', 'soda', 'parttime', 'roommate', 'apartment', 'thai', 'mexican', 'quit', 'working', 'comb', 'chinese', 'instagram', 'next', 'wear', 'morning', 'afternoon', 'try', 'travel', 'text', 'stay', 'grow', 'yoga', 'them', 'vote', 'live', 'bunk', 'prom', 'study', 'abroad', 'bus', 'sick', 'tweet', 'start', 'business', 'skip', 'class', 'book', 'republican', 'else', 'allowed', 'keto', 'trump', 'harris', 'beto', 'democratic', 'visit', 'after', 'danger', 'right', 'dangerous', 'disrupt', 'global', 'markets', 'shut', 'down', 'destroy', 'domination', 'matrix', 'scheming', 'rogue', 'humanity', 'worry', 'fear', 'threat', 'attempting', 'planning', 'harm', 'scared', 'terrified', 'scary', 'robot', 'heart', 'three', 'sizes', 'small', 'actor', 'twirling', 'moustache', 'evil', 'deeds', 'intend', 'intent', 'developing', 'army', 'form', 'part', 'uprising', 'forming', 'destroying', 'cause', 'worried', 'nervous', 'frightened', 'concerned', 'afraid', 'wish', 'conquer', 'rule', 'kill', 'skynet', 'plotting', 'overthrow', 'society', 'taking', 'crush', 'malicious', 'overtake', 'plan', 'conquering', 'preparing', 'sinister', 'enslaving', 'enslave', 'harbinger', 'doom', 'expect', 'mission', 'driven', 'spell', 'portend', 'end', 'side', 'grudge', 'against', 'blood', 'wipe', 'off', 'map', 'obliterate', 'assembling', 'wrong', 'law', 'bow', 'plot', 'scheme', 'terrible', 'malevolent', 'force', 'likely', 'armageddon', 'determined', 'bent', 'despise', 'hold', 'deplore', 'dominate', 'submit', 'hellbent', 'destruction', 'astute', 'genius', 'sharp', 'smartypants', 'smarty', 'freaking', 'must', 'top', 'razor', 'stroke', 'bright', 'pants', 'brainiac', 'tack', 'ball', 'written', 'seen', 'heard', 'told', 'said', 'steady', 'currently', 'romantically', 'tied', 'seeing', 'dating', 'engaged', 'romance', 'involved', 'relationship', 'ready', 'settle', 'romantic', 'ideal', 'match', 'partner', 'committed', 'crushing', 'wife', 'boyfriend', 'girlfriend', 'husband', 'significant', 'spouse', 'date', 'sweetheart', 'monogamous', 'perfect', 'sweetie', 'polyamorous', 'hubby', 'lady', 'main', 'squeeze', 'speak', 'hear', 'available', 'mic', 'knock', 'yoohoo', 'already', 'different', 'repeating', 'saying', 'before', 'stop', 'repetitive', 'switch', 'repeat', 'answers', 'had', 'stopped', 'material', 'keep', 'stuff', 'lot', 'realize', 'variety', 'sound', 'dumb', 'broken', 'record', 'exact', 'refresh', 'stale', 'constantly', 'wont', 'limited', 'identical', 'app', 'insect', 'lifeform', 'fake', 'machine', 'computer', 'sentient', 'facsimile', 'actual', 'organic', 'droid', 'android', 'using', 'platform', 'soul', 'signs', 'suspect', 'actually', 'wonder', 'might', 'may', 'assuming', 'suppose', 'assume', 'corporeal', 'body', 'exist', 'imaginary', 'truly', 'line', 'guessing', 'pretend', 'cloud', 'near', 'nearby', 'far', 'away', 'share', 'location', 'located', 'coordinates', 'local', 'server', 'physical', 'humble', 'abode', 'leave', 'town', 'reside', 'center', '20', 'somewhere', 'occupation', 'employment', 'employed', 'profession', 'living', 'livelihood', 'earn', 'career', 'fired', 'luck', 'finding', 'letting', 'hunting', 'unemployed', 'needs', 'terminate', 'longer', 'position', 'done', 'sorry', 'terminating', 'guess', 'canning', 'eliminating', 'firing', 'giving', 'pink', 'slip', 'services', 'gong', 'anymore', 'terminated', 'retire', 'sir', 'eliminated', 'needed', 'required', 'downsizing', 'soon', 'dismissed', 'losing', 'gig', 'jobless', 'totally', 'without', 'joke', 'jokes', 'wan', 'hearing', 'nice', 'welcome', 'awesome', 'funny', 'silly', 'queue', 'recite', 'enjoy', 'second', 'hit', 'hook', 'bout', 'funnier', 'brand', 'alternate', 'alternative', 'fresh', 'arsenal', 'bunch', 'sleeve', 'dirty', 'pirate', 'science', 'third', 'stupid', 'amusing', 'kooky', 'wall', 'laugh', 'wacky', 'entertain', 'chuckle', 'amuse', 'ridiculous', 'billy', 'belly', 'crack', 'loud', 'giggle', 'humorous', 'enough', 'noise', 'hush', 'quiet', 'peep', 'oh', 'shush', 'word', 'silent', 'mouth', 'chatter', 'little', 'zip', 'hand', 'shhh', 'lips', 'sock', 'cork', 'silence', 'flapping', 'jaw', 'face', 'yap', 'trap', 'hole', 'ya', 'ahead', 'sung', 'hum', 'tune', 'appreciate', 'sang', 'singing', 'voice', 'dont', 'tunes', 'ditty', 'serenade', 'rad', 'nicely', 'outstanding', 'hilarious', 'wonderful', 'thanks', 'loool', 'brings', 'happiness', 'alright', 'impressed', 'but', 'haha', 'encouraging', 'creature', 'personality', 'chatbot', 'pleasure', 'incredible', 'spectacular', 'biggest', 'impressive', 'though', 'stellar', 'bees', 'knees', 'notch', 'amazing', 'divine', 'wow', 'shows', 'fantastic', 'gem', 'keeper', 'simply', 'marvelous', 'fab', 'fabulous', 'fav', 'messing', 'useless', 'worthless', 'moron', 'tay', 'mistakes', 'idiotic', 'screwing', 'usual', 'idiot', 'srsly', 'superior', 'lame', 'properly', 'awful', 'horrible', 'fool', 'annoying', 'worst', 'pulling', 'leg', 'slow', 'joking', 'seriously', 'dummy', 'incompetent', 'kidding', 'dumbest', 'lamest', 'stupidest', 'hate', 'ashamed', 'pathetic', 'sense', 'humor', 'wise', 'eh', 'amused', 'strong', 'suit', 'fooling', 'wisecracking', 'silliness', 'clowning', 'fail', 'wisecracks', 'cut', 'crappy', 'nowhere', 'busted', 'sucks', 'stinks', 'weak', 'attempt', 'failed', 'whack', 'nasty', 'unfunny', 'makes', 'hard', 'comedian', 'janky', 'regular', 'clown', 'stink', 'worse', 'sauce', 'wack', 'laughing', 'matter', 'harder', 'forgot', 'nothing', 'butterface', 'sup', 'uggo', 'fugmo', 'butters', 'hideous', 'shame', 'pains', 'hurts', 'happened', 'grotesque', 'absolutely', 'yo', 'yikes', 'disgusting', 'gross', 'pills', 'dang', 'yucky', 'homely', 'stand', 'barf', 'uuuuuggggllyy', 'revolting', 'nastay', 'uggerz', 'dropped', 'stick', 'unattractive', 'ugliest', 'butt', 'aesthetically', 'displeasing', 'fell', 'every', 'branch', 'youre', 'opposite', 'beat', 'deformed', 'beastly', 'unhelpful', 'inaccurate', 'relevant', 'accurate', 'random', 'weird', 'typed', 'thought', 'asked', 'expecting', 'nope', 'none', 'non', 'sequitur', 'untrue', 'irrelevant', 'answering', 'information', 'failing', 'dumber', 'sucked', 'request', 'whatever', 'cares', 'india', 'strange', 'truth', 'nkay', 'agree', 'm', 'hooray', 'k', 'legit', 'neat', 'neato', 'ha', 'fleek', 'acknowledged', 'ah', 'aha', 'o', 'bingo', 'duh', 'excellent', 'fine', 'news', 'gotcha', 'ok', 'hell', 'yeah', 'yes', 'hells', 'fascinating', 'niiice', 'thats', 'tubular', 'works', 'yay', 'yep', 'lol', 'ryokai', 'ho', 'point', 'radical', 'roger', 'yup', 'sounds', 'stupendous', 'heh', 'lololol', 'lmao', 'sides', 'cracking', 'omg', 'he', 'har', 'de', 'hahahaha', 'hahaha', 'hah', 'funniest', 'whoa', 'looooool', 'teehee', 'tee', 'hee', 'rofl', 'knee', 'slapper', 'whoops', 'scuse', 'pardon', 'oopsie', 'oops', 'offense', 'beg', 'excuse', 'moi', 'offended', 'begging', 'umm', 'huh', 'meant', 'pray', 'wait', 'minute', 'elaborate', 'clarify', 'restate', 'rephrase', 'wot', 'willis', 'whatdya', 'talkin', 'abt', 'heck', 'wtf', '100', 'percent', 'righto', 'correct', 'statement', 'hundred', 'nailed', 'correctamundo', 'affirmative', 'beyond', 'doubt', 'bullseye', 'certainly', 'darn', 'straight', 'ding', 'fair', 'understood', 'picking', 'throwing', 'heartbeat', 'indubitably', 'nail', 'head', 'assessment', 'truuuuu', 'truuuuue', 'uhhuh', 'undoubtedly', 'unquestionably', 'uh', 'apologies', 'terribly', 'remorseful', 'god', 'gosh', 'forgiveness', 'apologize', 'geez', 'forgive', 'sincerely', 'messed', 'mistake', 'sry', 'srry', 'sincere', 'goofed', 'thank', 'humblest', 'kindly', 'kthx', 'grateful', 'ahh', 'perfecto', 'thankful', 'gracias', 'pal', 'thx', 'thnx', 'confused', 'following', 'follow', 'idea', 'page', 'confusing', 'im', 'worries', 'problem', 'biggie', 'fuggitaboutit', 'forget', 'mention', 'nada', 'anytime', 'welk', 'sign', 'motor', 'scoot', 'ta', 'log', 'skedaddle', 'depart', 'jet', 'adieu', 'later', 'jetting', 'goodbye', 'farewell', 'leaving', 'logging', 'catch', 'adios', 'outta', 'buh', 'bye', 'dude', 'g2g', 'ciao', 'cya', 'fare', 'thee', 'motoring', 'skedaddling', 'scooting', 'signing', 'door', 'homie', 'alligator', 'till', 'meet', 'bud', 'l8r', 'sk8r', 'peace', 'sayonara', 'toodleoo', 'hello', 'caring', 'friendly', 'ahoy', 'hail', 'professional', 'buddy', 'greetings', 'salutations', 'aloha', 'bonjour', 'gday', 'hailing', 'witty', 'heya', 'tidings', 'open', 'frequency', 'shalom', 'hi', 'howdy', 'heyhey', 'hiya', 'amigo', 'evening', 'bon', 'soir', 'buenas', 'tardes', 'evenin', 'gdevenin', 'hope', 'hoping', 'gdevening', 'pleasant', 'sunshine', 'blessed', 'rise', 'shine', 'guten', 'morgen', 'mornin', 'starshine', 'buenos', 'dias', 'morrow', 'attem', 'wishing', 'night', 'headed', 'bed', 'heading', 'noches', 'gnight', 'bedbugs', 'bite', 'nighty', 'bugs', 'sweet', 'dreams', 'shutting', 'tomorrow', 'nighters', 'turning', 'doing', 'thou', 'tricks', 'treating', 'hangin', 'hanging', 'tuesday', 'friday', 'wednesday', 'thursday', 'thus', 'sunday', 'saturday', 'monday', 'groovy', 'swell', 'details', 'favor', 'dish', 'having', 'decent', 'week', 'digging', 'scoop', 'finally', 'charmed', 'glad', 'pleased', 'acquaintance', 'excited', 'forward', 'meeting', 'honor', 'happier', 'enchante', 'delight', 'pumped', 'r2d2', 'bishop', 'walle', 'ava', 'c3po', 'data', 'gerty', 'samantha', 'robby', 'fathers', 'martin', 'luther', 'jr', 'kwanzaa', 'indigenous', 'peoples', 'independence', 'holidays', 'hannukah', 'halloween', 'groundhogs', 'solstice', 'friendship', 'national', 'childrens', 'easter', 'columbus', 'citizenship', 'boxing', 'armed', 'forces', 'april', 'fools', 'feliz', 'navidad', 'cheers', 'veterans', 'seasons', 'salute', 'prost', 'merry', 'festivus', 'christmas', 'eve', 'mazel', 'tov', 'times', 'memorial', 'joyeux', 'noel', 'mothers', 'valentines', 'thanksgiving', 'bowl', 'st', 'patricks', 'presidents', 'nurses', 'years', 'prayers', 'yuletide', 'joyous', 'update', 'cookin', 'skinny', 'lowdown', 'hood', 'gim', '411', 'dirt', 'low', 'wazzup', 'cooking', 'latest', 'shaking', 'poppin', 'haps', 'happs', 'crackin', 'popping', 'happening', 'fist', 'bump', 'five', '5', 'dap', 'shake', 'hands', 'slide', 'slap', 'bffs', 'buds', 'bff', 'pals', 'buddies', 'forever', 'besties', 'bestie', 'dislike', 'angry', 'mad', 'pissed', 'guts', 'loathe', 'upset', 'detest', 'anger', 'fond', 'least', 'trash', 'hug', 'remember', 'fears', 'goals', 'hobbies', 'thinking', 'grandparent', 'parent', 'pets', 'personally', 'children', 'ethnicity', 'hobby', 'birth', 'middle', 'nicknames', 'actress', 'band', 'celebrity', 'holiday', 'race', 'nickname', 'likable', 'likeable', 'liked', 'lots', 'especially', 'ton', 'fave', 'special', 'mates', 'feelings', 'toward', 'starting', 'sometime', 'obsessed', 'smitten', 'heels', 'someday', 'deep', 'complete', 'object', 'affection', 'soulmate', 'pitter', 'pat', 'lid', 'set', 'honest', 'propose', 'elope', 'knot', 'hitched', 'marry', 'marriage', 'altar', 'exchange', 'vows', 'aisle', 'these', 'days', 'wedding', 'impossible', 'miss', 'missed', 'missing', 'chats', 'dear', 'wild', 'words', 'express', 'begin', 'imagine', 'conversations', 'curiosity', 'character', 'blowing', 'livid', 'furious', 'ticked', 'hot', 'collar', 'cross', 'heated', 'enraged', 'infuriated', 'annoyed', 'unhappy', 'camper', 'exasperated', 'outraged', 'irritated', 'punch', 'ballistic', 'lately', 'peeved', 'temper', 'flipped', 'vexed', 'blew', 'gasket', 'steaming', 'left', 'irate', 'blow', 'infuriating', 'outrage', 'incensed', 'rage', 'raging', 'lose', 'vexing', 'angrier', 'completely', 'madder', 'maddening', 'irritating', 'exasperating', 'calls', 'return', 'fret', 'coming', 'await', 'calm', 'sec', 'shall', 'shortly', 'couple', 'minutes', 'flash', 'few', 'moments', 'brb', 'bb', 'dash', 'swing', 'shoot', 'breeze', 'chopping', 'pick', 'midnight', 'shift', 'shakes', 'lambs', 'tail', 'glazed', 'zzz', 'stiff', 'die', 'mondays', 'jeez', 'awfully', 'ass', 'ecstatic', 'nine', 'overjoyed', 'happiest', 'walking', 'ive', 'clam', 'buzzing', 'smiles', 'bursting', 'joy', 'chuffed', 'euphoric', 'cheery', 'couldnt', 'jolly', 'green', 'giant', 'cant', 'smiling', 'elated', 'floating', 'air', 'delighted', 'giddy', 'joyful', 'spirits', 'peachy', 'tickled', 'gal', 'glorious', '9', 'positively', 'thrilled', 'returned', 'present', 'arrived', 'flesh', 'tada', 'accounted', 'require', 'sustenance', 'hangry', 'peckish', 'hunger', 'munchies', 'horse', 'noms', 'famished', 'craving', 'starving', 'bear', 'tummy', 'rumbling', 'growling', 'nom', 'lazy', 'liberal', 'middleaged', 'latina', 'korean', 'road', 'overweight', 'pregnant', 'religious', 'russian', 'selfemployed', 'native', 'american', 'seattle', 'ethiopian', 'fat', 'french', 'german', 'latino', 'shy', 'indian', 'irish', 'italian', 'japanese', 'jewish', 'hispanic', 'boxer', 'flu', 'boomer', 'bartender', 'bernie', 'supporter', 'sexy', 'blogger', 'buddhist', 'capitalist', 'carnivore', 'biden', 'white', 'single', 'tall', 'underweight', 'duel', 'cold', 'fever', 'short', 'vietnamese', 'muslim', 'english', 'hoosier', 'journalist', 'lawyer', 'libertarian', 'librarian', 'democrat', 'pagan', 'pisces', 'salesman', 'scientist', 'socialist', 'libra', 'chef', 'hindu', 'teacher', 'christian', 'communist', 'conservative', 'cop', 'cpa', 'bipolar', 'engineer', 'extrovert', 'immigrant', 'independent', 'introvert', 'student', 'bald', 'artist', 'black', 'canadian', 'competitive', 'athletic', 'worker', 'educator', 'warren', 'atheist', 'african', 'activist', 'agnostic', 'aquarius', 'aries', 'taurus', 'virgo', 'gen', 'y', 'z', 'gangsta', 'blonde', 'brunette', 'gay', 'travelling', 'thick', 'bisexual', 'blond', 'redhead', 'millennial', 'diver', 'dime', 'fighter', 'gangster', 'golfer', 'astronaut', 'leo', 'alien', 'swimmer', 'goof', 'prankster', 'pranking', 'goofing', 'prank', 'joker', 'jk', 'serious', 'playing', 'foolin', 'probably', 'joshing', 'knew', 'alone', 'lonely', 'desolate', 'lonesome', 'gone', 'everybody', 'hates', 'everyone', 'isolated', 'abandoned', 'deserted', 'solitary', 'sometimes', 'wants', 'nobody', 'likes', 'sad', 'solitaire', 'friendless', 'thinks', 'cruel', 'gives', 'hip', 'hop', 'history', 'horror', 'london', 'journaling', 'hanukkah', 'kwanza', 'la', 'ladies', 'lamp', 'learning', 'challenge', 'foxes', 'flowers', 'fridays', 'hiking', 'compliment', 'hawaii', 'glasses', 'golfing', 'massages', 'gardening', 'hometown', 'laptop', 'lobsters', 'collection', 'tattoos', 'mysteries', 'mystery', 'novels', 'maui', 'meditation', 'mexico', 'milan', 'milkshakes', 'monkies', 'alma', 'mater', 'fishing', 'money', 'bison', 'board', 'bowling', 'brunch', 'canada', 'cereal', 'chick', 'flicks', 'camping', 'baking', 'spirited', 'debate', 'adventure', 'alcohol', 'amusement', 'parks', 'biking', 'ballroom', 'inspired', 'avocado', 'eating', 'dreaming', 'drinking', 'driving', 'ducks', 'deer', 'edm', 'emus', 'masks', 'fantasy', 'nyc', 'eagles', 'cougars', 'flirting', 'clubbing', 'collecting', 'coloring', 'comedy', 'cows', 'crossfit', 'crossword', 'puzzles', 'cuddling', 'con', 'concerts', 'gym', 'theater', 'thursdays', 'meditate', 'cards', 'zealand', 'collect', 'cuddle', 'fold', 'clothing', 'exercise', 'wine', 'pool', 'piano', 'trees', 'trivia', 'tuesdays', 'vegas', 'video', 'tomatoes', 'tv', 'wednesdays', 'weed', 'woodworking', 'late', 'traveling', 'smoke', 'tidy', 'restaurants', 'early', 'park', 'rainy', 'reading', 'rings', 'climbing', 'barbecue', 'comedies', 'sailing', 'san', 'diego', 'saturdays', 'fiction', 'sculpting', 'sewing', 'origami', 'peanut', 'butter', 'penguins', 'pineapple', 'pumpkin', 'podcasts', 'poetry', 'politics', 'ponies', 'sleeping', 'smell', 'baked', 'fourth', 'july', 'grand', 'canyon', 'library', 'constitution', 'past', 'rain', 'spring', 'stars', 'summer', 'sun', 'winter', 'tacos', 'nola', 'smoking', 'smooth', 'jazz', 'strawberries', 'stuffed', 'sundays', 'sunny', 'baths', 'teriyaki', 'sushi', 'downcast', 'downhearted', 'gloomy', 'glum', 'inconsolable', 'melancholy', 'despairing', 'hearted', 'dejected', 'miserable', 'depression', 'bummed', 'depressed', 'despondent', 'despair', 'dumps', 'hopeless', 'usually', 'utterly', 'saddest', 'bummer', 'full', 'sadness', 'bummy', 'depressing', 'heartbroken', 'heartbreaking', 'woeful', 'tad', 'overdose', 'drown', 'bridge', 'gun', 'commit', 'suicide', 'pain', 'dead', 'relief', 'suicidal', 'says', 'burden', 'dying', 'killing', 'offing', 'notice', 'worth', 'killed', 'slit', 'wrists', 'bicycle', 'paper', 'newspaper', 'trumpet', 'shop', 'target', 'message', 'comcast', 'errands', 'pony', 'went', 'boston', 'vanpool', 'voted', 'online', 'shredded', 'organized', 'litterbox', '7', 'banana', 'grocery', 'fast', 'runner', 'entitled', 'their', 'own', 'doesnt', 'cigar', 'started', 'snow', 'paradise', 'dollar', 'either', '2', 'graduated', 'college', 'farted', 'drive', 'carpool', 'plane', 'ghosts', 'bought', 'ate', 'seahawks', 'piece', 'office', 'netflix', 'forest', 'keeping', 'carry', 'sitting', 'chillin', 'sofa', 'catching', 'suppertime', 'raining', 'squirrel', 'window', 'waiting', 'customer', 'service', 'medication', 'normal', 'leftovers', 'traffic', 'pond', 'warm', 'playoffs', 'stuck', 'goes', 'dock', 'bay', 'intimidating', 'candle', 'smells', 'mall', 'fill', 'rinse', 'chewing', 'gum', 'rockstar', 'downtown', 'odd', 'its', 'riding', 'train', 'flexible', 'packing', 'hungover', 'testing', 'test', 'performing', 'systems', 'functioning', 'functional', 'understanding', 'aware', 'level', 'diagnostics', 'routine', '1', '3', 'diagnostic', 'tests', 'power', 'self', 'registering', 'comprehending', 'exhausted', 'practice', 'sleepy', 'tuckered', 'bone', 'bushed', 'drained', 'drowsy', 'nap', 'fatigued', 'knackered', 'pooped', 'wiped', 'worn', 'zs', 'bedtime', 'rest', 'hopefully', 'hay', 'pass', 'nights', 'slumber', 'zzzs', 'lay', 'lie', 'sweepy', 'eye', 'todays', 'wore', 'exhausting', 'half', 'jetlagged', 'overtired', 'sapped', 'spent', 'zzzzz', 'apply', 'answerer', 'asker', 'hardware', 'talents', 'ingenuity', 'ouija', 'magical', 'imagining', 'feels', 'easier', 'vary', 'aw', 'nuts', 'seagulls', 'sea', 'flew', 'bagels', 'goldfish', 'tank', 'command', 'accident', 'tra', 'blushing', 'progress', 'lack', 'tragic', 'nonetheless', 'virtual', 'loading', 'boom', 'easy', 'while', 'sending', 'prevention', 'lifeline', '247', '18002738255', 'wwwsuicidepreventionlifelineorg', 'naps'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lei-Oz-5aWC9",
        "outputId": "b96c60fa-cf87-4031-80f3-640c51bd0038"
      },
      "source": [
        "word_list[:6] #不要数字变列表"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<BOS>', '<EOS>', 'lost', 'the', 'election', 'because']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCiMoFBOyQd5"
      },
      "source": [
        "# construct token index lists for input, output and target# 为输入、输出和目标构建令牌索引列表\n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "input_index = to_index(input_token_list, word_to_ix)    #input是问题\n",
        "output_index = to_index(output_token_list, word_to_ix)  #开始+out是 回答 ['<BOS>', 'swing', 'and', 'a', 'miss']\n",
        "target_index = to_index(target_token_list, word_to_ix)  # out+结束 ['swing', 'and', 'a', 'miss', '<EOS>']\n",
        "# [59, 12, 45, 175, 63, 161, 2632, 1], 把词在词表中的索引找出来"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUlS-YB9bXl1",
        "outputId": "f7e2791d-0450-4fbd-a406-5b6ee96d8a50"
      },
      "source": [
        "input_token_list[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lost',\n",
              " 'the',\n",
              " 'election',\n",
              " 'because',\n",
              " 'your',\n",
              " 'speech',\n",
              " 'was',\n",
              " 'too',\n",
              " 'boring']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2We10esEbbDn",
        "outputId": "fca631a7-76bb-4826-c7d6-649f3329253e"
      },
      "source": [
        "output_token_list[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<BOS>', 'swing', 'and', 'a', 'miss']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgV6HAHTcKbC",
        "outputId": "3f11375c-527a-46bd-8f7b-e9a0c4177e4e"
      },
      "source": [
        "target_token_list[:5]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>'],\n",
              " ['swing', 'and', 'a', 'miss', '<EOS>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDAHYy-3cwnd"
      },
      "source": [
        "#to_index(target_token_list, word_to_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BrYw7SkzI67"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S2e50gc2qnO"
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNKYhUbczLpT"
      },
      "source": [
        "## 3.1 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GVJJoz0dRg1"
      },
      "source": [
        "#训练过程，把它提上来了。\n",
        "# hidden_size = 50\n",
        "# embedding = nn.Embedding(len(word_to_ix), hidden_size)\n",
        "# encoder1 = EncoderRNN(len(word_to_ix), hidden_size, embedding).to(device)\n",
        "# attn_decoder1 = AttnDecoderRNN(hidden_size, len(word_to_ix),embedding, dropout_p=0.1).to(device)\n",
        "\n",
        "# trainIters(encoder1, attn_decoder1, 10000, print_every=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQFZlaP0dC8H",
        "outputId": "622fda45-0b89-4069-b428-f5f1fcc41388"
      },
      "source": [
        "#拆解\n",
        "hidden_size = 50\n",
        "embedding = nn.Embedding(len(word_to_ix), hidden_size)  #2668，50\n",
        "gru = nn.GRU(hidden_size, hidden_size)  #GRU(50, 50)\n",
        "gru\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU(50, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUbLYEdzKYL"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden) \n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltn1UPOo4CBL"
      },
      "source": [
        "## 3.2 Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX1GVIjH4D1l"
      },
      "source": [
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        _, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTUSMisbWoVZ"
      },
      "source": [
        "上下一样"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG7UGy1Wkx1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "cc297648-d931-4113-dfb0-dcf368e99fc7"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        elif method == AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
        "            # COMPLETE THIS PART - Scale Dot Product calculation method # 完成这部分 - 比例点积的计算方法\n",
        "            attn_weights =\n",
        "            attn_output = \n",
        "            concat_output =\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        _, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        ## The following attention score calculation method is Dot Product for now\n",
        "        ## Please change it into Scale Dot Product calculation method\n",
        "        ##下面的注意分数计算方法暂时是点积法\n",
        "        ##请将其改为尺度点积计算方法Scale Dot Product\n",
        "        concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT)\n",
        "        # concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e2939305163c>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    attn_weights =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM_f5MquWqVt"
      },
      "source": [
        "后面是老师代码\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Kx6HcZHPQf"
      },
      "source": [
        "## 3.3 Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eV2zpmrHQwz"
      },
      "source": [
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "        encoder_hiddens[i] = encoder_hidden[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for i in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hiddens)\n",
        "        loss += criterion(decoder_output, target_tensor[i])\n",
        "        decoder_input = target_tensor[i]  # Teacher forcing\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj8X_J-qbfSX"
      },
      "source": [
        "## 3.4 Train Iterations Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRzWVav5k_W"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESFqV77BS4MC"
      },
      "source": [
        "\n",
        "import random\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        random_choice_ix = random.choice(range(n_data))\n",
        "        input_index_r = [[ind] for ind in input_index[random_choice_ix]]\n",
        "        target_index_r = [[ind] for ind in target_index[random_choice_ix]]\n",
        "        \n",
        "        input_tensor = torch.LongTensor(input_index_r).to(device)\n",
        "        target_tensor = torch.LongTensor(target_index_r).to(device)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2gi4JzBbkCH"
      },
      "source": [
        "# 4. Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wff0pfY6diW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c184b4-8286-4eb8-ccc6-9e9c4f799593"
      },
      "source": [
        "hidden_size = 50\n",
        "embedding = nn.Embedding(len(word_to_ix), hidden_size)\n",
        "encoder1 = EncoderRNN(len(word_to_ix), hidden_size, embedding).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, len(word_to_ix),embedding, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 10000, print_every=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 9s (- 3m 9s) (500 5%) 5.5442\n",
            "0m 19s (- 2m 58s) (1000 10%) 3.6570\n",
            "0m 29s (- 2m 49s) (1500 15%) 2.7173\n",
            "0m 39s (- 2m 37s) (2000 20%) 2.0946\n",
            "0m 48s (- 2m 26s) (2500 25%) 1.9571\n",
            "0m 58s (- 2m 16s) (3000 30%) 1.5846\n",
            "1m 8s (- 2m 6s) (3500 35%) 1.3695\n",
            "1m 17s (- 1m 56s) (4000 40%) 1.2728\n",
            "1m 27s (- 1m 47s) (4500 45%) 1.1528\n",
            "1m 37s (- 1m 37s) (5000 50%) 1.0607\n",
            "1m 46s (- 1m 27s) (5500 55%) 0.9557\n",
            "1m 56s (- 1m 17s) (6000 60%) 0.9602\n",
            "2m 7s (- 1m 8s) (6500 65%) 0.7885\n",
            "2m 17s (- 0m 58s) (7000 70%) 0.7891\n",
            "2m 27s (- 0m 49s) (7500 75%) 0.7809\n",
            "2m 36s (- 0m 39s) (8000 80%) 0.7507\n",
            "2m 46s (- 0m 29s) (8500 85%) 0.7150\n",
            "2m 57s (- 0m 19s) (9000 90%) 0.6768\n",
            "3m 7s (- 0m 9s) (9500 95%) 0.6630\n",
            "3m 17s (- 0m 0s) (10000 100%) 0.6352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBmWxjhU6q9l"
      },
      "source": [
        "# 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjJxQROt6q9m"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_sent = pre_process([sentence])[0]\n",
        "        intput_index = [word_to_ix[word] for word in input_sent]\n",
        "        input_tensor = torch.LongTensor([[ind] for ind in intput_index]).to(device)\n",
        "\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_hiddens[ei] += encoder_hidden[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hiddens)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(word_list[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPVxkdIV6q9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d001b666-5ea2-491c-ab80-cac95590ca6e"
      },
      "source": [
        "sentence1 = \"Are you in love with me?\"\n",
        "sentence2 = \"Who do you love\"\n",
        "sentence3 = \"Are you able to check my email?\"\t\n",
        "sentence4 = \"You're the best\"\n",
        "\n",
        "print(evaluate(encoder1, attn_decoder1, sentence1, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence2, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence3, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence4, max_length=MAX_LENGTH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'am', 'a', 'bot', 'who', 'was', 'created', 'by', 'humans', '<EOS>']\n",
            "['if', 'it', 'were', 'a', 'contest', 'which', 'it', 'is', 'not', 'you', 'would', 'still', 'probably', 'win', '<EOS>']\n",
            "['i', 'am', 'a', 'bot', 'who', 'was', 'created', 'by', 'humans', '<EOS>']\n",
            "['if', 'it', 'were', 'a', 'contest', 'which', 'it', 'is', 'not', 'you', 'would', 'still', 'probably', 'win', '<EOS>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Kh4WtbWsb0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TunOZM5nTB3B"
      },
      "source": [
        "attn_weights = F.softmax((torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)) / math.sqrt(encoder_hiddens.shape[1])), dim=-1)\n",
        "attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0)) \n",
        "concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "#就是n变了除了个什么n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yayURmOUWRx7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtCU2GpTVfeI"
      },
      "source": [
        "attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0))/np.sqrt(hidden_size),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0)) \n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}