{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZEqErhBFo/wH8cihzYU77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/E10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho6eix47qVfr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKVORGV1ExAk"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhU6WFcEzZr"
      },
      "source": [
        "## E1. Explain the difference between global and local attention with examples. \n",
        "\n",
        "**NOTE: Your E1 will NOT be marked if no example provided**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MbWlHIQab1"
      },
      "source": [
        "E1. 举例说明全局注意和局部注意之间的区别。 【例子】\n",
        "注意：如果没有提供例子，你的E1将不会被评分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLtGnMhHE1IS"
      },
      "source": [
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfV3j4_ukfFg"
      },
      "source": [
        "## E2. Scale Dot Product\n",
        "Please change the **Dot Product** attention in the following code into **Scaled Dot Product** attention\n",
        "\n",
        "**Dot Product:**\n",
        "\n",
        "![Dot_Product](https://drive.google.com/uc?id=1QtBgCp53e_6A_vzaMFEo89GJbTxnXagJ)\n",
        "\n",
        "**Scaled Dot Product:**\n",
        "\n",
        "![Scaled_Dot_Product](https://drive.google.com/uc?id=1v6n9WChBVfy0mBG2yxK9MUvGKzVGCmOt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltewOo1OQgeY"
      },
      "source": [
        "比例点积\n",
        "请将以下代码中的点积关注点改为比例点积关注点\n",
        "【dot-product---> scaled dot product】\n",
        "点积。\n",
        "缩放点积。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG7UGy1Wkx1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "cc297648-d931-4113-dfb0-dcf368e99fc7"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        elif method == AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
        "            # COMPLETE THIS PART - Scale Dot Product calculation method # 完成这部分 - 比例点积的计算方法\n",
        "            attn_weights =\n",
        "            attn_output = \n",
        "            concat_output =\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        _, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        ## The following attention score calculation method is Dot Product for now\n",
        "        ## Please change it into Scale Dot Product calculation method\n",
        "        ##下面的注意分数计算方法暂时是点积法\n",
        "        ##请将其改为尺度点积计算方法Scale Dot Product\n",
        "        concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT)\n",
        "        # concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e2939305163c>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    attn_weights =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}