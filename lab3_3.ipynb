{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3.3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkV62RqNIfBsloA006m/Jy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/lab3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IMoIu2fp-VK"
      },
      "source": [
        "'''\n",
        "PyTorch nn.module 和toch.optim 实现CBOW [多到一]\n",
        "\n",
        "我们会nn.Module 和torch.optim 来训练word2Vec skip-gram\n",
        "=======\n",
        "\n",
        "1：使用 nn.Module 和优化器 torch.optim 去训练 word2vec CBOW [这个地方变了]\n",
        "window_size = 1 ; embedding_size = 2 (可研究nn.Embedding，不必要)\n",
        "\n",
        "2plot 每个词。\n",
        "\n",
        "潜入大小2； 提供了预处理和超参数设置的代码\n",
        "\n",
        "'''\n",
        "'''\n",
        "nn.model 就是一个个转头堆成神经网络\n",
        "而word2vec 就需要两个线性回归就可做出来了\n",
        "\n",
        "linear （参数 神经元：work——size——hidden.size 到vecbolary.size)\n",
        "window就是为了做中center和context的呀\n",
        "\n",
        "nn.model()类\n",
        "\n",
        "在学习nn.model()类之前，先简要了解一下nn。\n",
        "nn主要有四个模块\n",
        "\n",
        "nn.Parameter: 一个张量的子类，用于表示可学习的参数 w， b\n",
        "nn.Module： 网络层的基类，用于管理网络的属性，LeNet是一个module类，LeNet的子模块例如conv2，也是一个nn.module类\n",
        "nn.functional：用于函数的实现，比如卷积运算，加法运算\n",
        "nn.__init__：参数初始化方法\n",
        "nn.Module()类的主要属性\n",
        "\n",
        "parameter ： 用于存储和管理Parameter类\n",
        "Module ： 用于存储和管理Module类相关\n",
        "buffers ：存储缓冲属性，比如均值等\n",
        "其他五个是用于管理钩子函数（_hocks()\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETfWhLaIqV7R"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from random import shuffle\n",
        "import pprint\n",
        "\n",
        "# Raw data - sentences\n",
        "# Let's create toy data for simplicity \n",
        "sentences = [\"he likes cat\",\n",
        "             \"he likes dog\",\n",
        "             \"he likes animal\",\n",
        "             \"dog cat animal\",\n",
        "             \"she likes cat\",\n",
        "             \"she dislikes dog\",\n",
        "             \"cat likes fish\",\n",
        "             \"cat likes milk\",\n",
        "             \"dog likes bone\",\n",
        "             \"dog dislikes fish\",\n",
        "             \"dog likes milk\",\n",
        "             \"she likes movie\",\n",
        "             \"she likes music\",\n",
        "             \"he likes game\",\n",
        "             \"he likes movie\",\n",
        "             \"cat dislikes dog\"]\n",
        "\n",
        "# convert all sentences to unique word list\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "\n",
        "\n",
        "# make dictionary so that we can reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# make window size=1 for cbow\n",
        "# i.e.) he likes cat\n",
        "#   -> ([likes], he), ([he, cat], likes), ([likes], cat)\n",
        "#   -> ([likes, likes], he), ([he, cat], likes), ([likes, likes], cat)\n",
        "# Double the input when the word doesn't have two neighbours\n",
        "# This will make your input have the same size, which will make it easier when you write the CBOW model code\n",
        "# But this trick only works when window_size = 1\n",
        "\n",
        "cbow = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence = sentence.split()\n",
        "    for i in range(len(sentence)):\n",
        "        centre = word_dict[sentence[i]]\n",
        "        if i > 0 and i < len(sentence)-1:\n",
        "            context = [word_dict[sentence[i - 1]], word_dict[sentence[i + 1]]]\n",
        "        elif i == 0:\n",
        "            context = [word_dict[sentence[i + 1]], word_dict[sentence[i + 1]]]\n",
        "        else:\n",
        "            context = [word_dict[sentence[i - 1]], word_dict[sentence[i - 1]]]\n",
        "\n",
        "        cbow.append([context, centre])\n",
        "\n",
        "# hyperparameter\n",
        "voc_size = len(word_list)\n",
        "learning_rate = 0.1\n",
        "batch_size = 16\n",
        "embedding_size = 2\n",
        "no_of_epochs = 5000"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "pszi9SaKvSO4",
        "outputId": "68396521-2f62-4de5-a5d4-2ae827366753"
      },
      "source": [
        "#prepare batch\n",
        "\n",
        "voc_size = len(word_list)   #13\n",
        "\n",
        "def prepare_batch(data_temp):\n",
        "    x1 = []   #window1\n",
        "    x2 = [] #window2 context\n",
        "    #data_temp = cbow\n",
        "\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    inputs1 = []\n",
        "\n",
        "    for i in range(len(data_temp)):  # data = data_temp\n",
        "      input1_temp = [0]*voc_size\n",
        "      input1_temp[data_temp[i][0][0]] = 1     #48  第一个context词  \n",
        "      x1.append(input1_temp)      #48\n",
        "      labels.append(data_temp[i][1])  #中心词的位置标记   48 那么这个labels不就是output层么\n",
        "\n",
        "    #for k in range(len(data_temp)): \n",
        "      input2_temp = [0]*voc_size\n",
        "      input2_temp[data_temp[i][0][1]] = 1   #第二个context 词 48\n",
        "      x2.append(input2_temp)  \n",
        "\n",
        "      inputs1.append(x1[i])\n",
        "      inputs1.append(x2[i])\n",
        "      inputs.append(inputs1)\n",
        "      inputs1 = []  \n",
        "\n",
        "    return np.array(inputs), np.array(labels)\n",
        "'''\n",
        "pre = prepare_batch(cbow)\n",
        "pprint.pprint(pre)\n",
        "print(pre[0])\n",
        "#pre[0][0][0] 13 one-h pre[0][0] 2 两个context词的one-hot pre[0] 48  [12,12]9 有48对。没对有x1（12） x2（12）label 就是central index\n",
        "\n",
        "#x1 48 个第一位的one-hot  x2 48个 第二位的one-hot  label就是中心词的字典indexß\n",
        "'''"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\npre = prepare_batch(cbow)\\npprint.pprint(pre)\\nprint(pre[0])\\n#pre[0][0][0] 13 one-h pre[0][0] 2 两个context词的one-hot pre[0] 48  [12,12]9 有48对。没对有x1（12） x2（12）label 就是central index\\n\\n#x1 48 个第一位的one-hot  x2 48个 第二位的one-hot  label就是中心词的字典indexß\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIhSUK4k2u_r",
        "outputId": "bb573876-7ac8-4f2c-d6d9-e4a94a8fb8ec"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from random import shuffle\n",
        "\n",
        "class Cbow_linear(nn.Module):\n",
        "  def __init__(self,inputs, outputs):\n",
        "    super(Cbow_linear,self).__init__()\n",
        "    self.linear1 = nn.Linear(voc_size, embedding_size)\n",
        "    self.linear2 = nn.Linear(embedding_size, voc_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.linear1(x[:,0])\n",
        "    x2 = self.linear1(x[:,1])\n",
        "    x = (F.relu(x1) + F.relu(x2))/2\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    return x\n",
        "    \n",
        "model = Cbow_linear(voc_size, embedding_size)  # model = cbow_net\n",
        "print(model)\n"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cbow_linear(\n",
            "  (linear1): Linear(in_features=13, out_features=2, bias=True)\n",
            "  (linear2): Linear(in_features=2, out_features=13, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Ev3jWeqrvB",
        "outputId": "953a0a03-327f-47f9-9cff-dd8b98820fa9"
      },
      "source": [
        "#要\n",
        "\n",
        "#opt\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimiser = optim.SGD(model.parameters(), lr=learning_rate)#, momentum=0.9\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  shuffle(cbow)\n",
        "  loss_sum = 0\n",
        "\n",
        "  for ind in range(0,len(cbow),batch_size):     #ind: 0,16,32\n",
        "    data_temp = cbow[ind : min(ind+batch_size,len(cbow))] # 3次了已经\n",
        "    inputs_temp, labels_temp = prepare_batch(data_temp)\n",
        "\n",
        "    inputs_torch = torch.from_numpy(inputs_temp).float()\n",
        "    labels_torch = torch.from_numpy(labels_temp)\n",
        "\n",
        "    \n",
        "\n",
        "    log_softmax = F.log_softmax(model.forward(inputs_torch),dim = 1)\n",
        "    loss = F.nll_loss(log_softmax, labels_torch)\n",
        "    \n",
        "    #GD\n",
        "    optimiser.zero_grad()#j\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    loss_sum += loss.item()#没有这部就全是0\n",
        "\n",
        "  if epoch % 500 == 499: \n",
        "    print('Epoch: %d, loss: %.4f' %(epoch + 1, loss_sum))\n",
        "\n",
        "\n",
        "\n",
        "    #pred\n",
        "    #Cbow_linear(nn.Module)\n"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 500, loss: 5.0984\n",
            "Epoch: 1000, loss: 4.8292\n",
            "Epoch: 1500, loss: 4.6646\n",
            "Epoch: 2000, loss: 4.5700\n",
            "Epoch: 2500, loss: 4.5169\n",
            "Epoch: 3000, loss: 4.4852\n",
            "Epoch: 3500, loss: 4.4607\n",
            "Epoch: 4000, loss: 4.4427\n",
            "Epoch: 4500, loss: 4.4332\n",
            "Epoch: 5000, loss: 4.4117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCCMT1INgE8J",
        "outputId": "f7667d63-3313-45d0-81d7-9e01fe57833c"
      },
      "source": [
        "trained_embeddings.shape"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "TCnMHYN_evOd",
        "outputId": "55140183-d9d4-470a-b193-2a7dfc465c4c"
      },
      "source": [
        "# Get the trained projection matrix\n",
        "trained_embeddings = model.linear1.weight.view(13,2).data.numpy()\n",
        "# Visualise result\n",
        "for i, label in enumerate(word_list):\n",
        "    x, y = trained_embeddings[i]\n",
        "    # print (label, \" : \", x, \" \" , y) # uncomment to see the detailed vector info\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
        "                 textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-LowWmgRPVS",
        "outputId": "d3a9715e-0755-4b28-e129-67dfdfde59f7"
      },
      "source": [
        "#no\n",
        "inputs_torch.shape#torch.Size([16, 2, 13])--- #希望这里是【16，13】\n",
        "labels_torch.shape  #torch.Size([16])  \n",
        "log_softmax.shape#torch.Size([16, 2, 13])"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 2, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c62gBu9zc5C4"
      },
      "source": [
        ""
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZpgwI3GJ7JU"
      },
      "source": [
        "\n",
        "'''\n",
        "for ind in range(0,len(cbow),batch_size):     #ind: 0,16,32\n",
        "  data_temp = cbow[ind : min(ind+batch_size,len(cbow))]  #batch每次录入三个\n",
        "  \n",
        "  inputs_temp, labels_temp = prepare_batch(data_temp)\n",
        "#pprint.pprint(data_temp)##batch 是16么内容应该是16\n",
        "#pprint.pprint(labels_temp) #array([ 9, 12,  0,  3,  0,  2,  6,  0,  6,  2,  8, 12,  6, 11,  7,  5])\n",
        "#pprint.pprint(inputs_temp)#16个 两个内容词\n",
        "\n",
        "  inputs_torch = torch.from_numpy(inputs_temp).float()\n",
        "  labels_torch = torch.from_numpy(labels_temp)\n",
        "\n",
        "  log_softmax = F.log_softmax(model.forward(inputs_torch),dim = 1)\n",
        "  loss = F.nll_loss(log_softmax, labels_torch)\n",
        "\n",
        "\n",
        "  \n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      W1.data -= learning_rate*W1.grad.data\n",
        "      Wout.data -= learning_rate*Wout.grad.data\n",
        "  W1.grad.data.zero_()\n",
        "  Wout.grad.data.zero_()\n",
        "\n",
        "  loss_sum += loss.item()\n",
        "\n",
        "if epoch % 500 == 499: \n",
        "    print('Epoch: %d, loss: %.4f' %(epoch + 1, loss_sum))\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "LyTdfQGTbDo-",
        "outputId": "379bfdbf-555a-48d6-ec83-4c54378002de"
      },
      "source": [
        "    #input = model.forward(inputs_torch)\n",
        "'''\n",
        "print(inputs_torch)\n",
        "print(inputs_torch[:,0,:])\n",
        "print(inputs_torch[:,1])\n",
        "'''\n",
        "  #print(input[:,0])\n",
        "  #print(input[0])\n",
        "'''\n",
        "\n",
        "[:,:,0]\n",
        "取三维数组中第1列的所有楼层，所有行的数据\n",
        "#https://blog.csdn.net/u014159143/article/details/80307717?ops_request_misc=&request_id=&biz_id=102&utm_term=python%20%5B%20：，0，：%5D什么意思&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-80307717.first_rank_v2_pc_rank_v29\n",
        "\n",
        "'''"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n[:,:,0]\\n取三维数组中第1列的所有楼层，所有行的数据\\n#https://blog.csdn.net/u014159143/article/details/80307717?ops_request_misc=&request_id=&biz_id=102&utm_term=python%20%5B%20：，0，：%5D什么意思&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-80307717.first_rank_v2_pc_rank_v29\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47go9HJJI2__",
        "outputId": "8363b6cc-b643-4f3a-b060-db23c968fa89"
      },
      "source": [
        "ind+batch_size\n",
        "#min(0+16, 48 )--16_____0---16\n",
        "#(16+16,48)32-______16-32\n",
        "#32+16,48 48____32---48\n",
        "\n",
        "#https://blog.csdn.net/wangchuner/article/details/102829111?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161684141016780265451641%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161684141016780265451641&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-2-102829111.first_rank_v2_pc_rank_v29&utm_term=如何调用类里面的函数+python\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuUEy0EkIdOs"
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3f8CsTVtG8z"
      },
      "source": [
        "#optim\n",
        "#https://blog.csdn.net/nineship/article/details/89228497?ops_request_misc=&request_id=&biz_id=102&utm_term=torch.optim%20怎么用&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-89228497.first_rank_v2_pc_rank_v29"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}