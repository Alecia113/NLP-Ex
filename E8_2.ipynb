{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E8.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvWjfFbyiFP+flM9QpbfcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/E8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgLuJj2TcdKn"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xLcqJbBcip9"
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences \n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4etbcvxCcpLk",
        "outputId": "5e8f2d6d-05a6-4303-fbaa-8a9507645525"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)  \n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])\n",
        "        # The reason we are using log_softmax here is that we want to calculate -log(p) and find the minimum score                    \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)      \n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):  \n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()       \n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, training loss: 3.6821, training acc: 10.42%\n",
            "Epoch: 20, training loss: 3.4591, training acc: 14.58%\n",
            "Epoch: 30, training loss: 3.0505, training acc: 14.58%\n",
            "Epoch: 40, training loss: 2.6286, training acc: 20.83%\n",
            "Epoch: 50, training loss: 2.2385, training acc: 45.83%\n",
            "Epoch: 60, training loss: 1.9233, training acc: 62.50%\n",
            "Epoch: 70, training loss: 1.6583, training acc: 70.83%\n",
            "Epoch: 80, training loss: 1.4220, training acc: 79.17%\n",
            "Epoch: 90, training loss: 1.2126, training acc: 85.42%\n",
            "Epoch: 100, training loss: 1.0376, training acc: 89.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pObgeE13hcRB"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLbdnwsNcrEO",
        "outputId": "e43839e6-7815-45cf-fd64-9241774d00ce"
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# To-Do: modify this function\n",
        "# Generate text, currently only works with k=1 \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "            if k == 1:\n",
        "              id, s = get_topK(predicted, k)[0]\n",
        "              # get the output word\n",
        "              output_word = ind_to_word(id)\n",
        "              # put the word into the sentence input\n",
        "              # calcualte the accumulated score by -log(p)\n",
        "              successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "            else:\n",
        "              # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "              # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "        \n",
        "              id_n =[]\n",
        "              s_n = []\n",
        "              for j in range(k):            \n",
        "              #j = 1,2 3\n",
        "                #get_topK(predicted, k)[0]  #取三个最大的【（）（）（）】list【0】第一个【1】第二个【2】第三个 id 和s 应该是list\n",
        "                id_n.append(get_topK(predicted, k)[j][0])\n",
        "                s_n.append(get_topK(predicted, k)[j][1])\n",
        "                #get_topK(predicted, 3)[0][1]    ===s  [0][0]==id\n",
        "                #id, s = get_topK(predicted, k)[k] 取出【0】第一个输入反馈个index和值\n",
        "              # get the output word\n",
        "\n",
        "                output_word = ind_to_word(id_n[j])   #给出个单词\n",
        "              # put the word into the sentence input\n",
        "              # calcualte the accumulated score by -log(p)\n",
        "                successives.append((seed_text + ' ' + output_word, score - s_n[j])) #给出个结果句子+分数。\n",
        "                #是把这句话添加到里面去了\n",
        "            # max_s = successives[0][1]\n",
        "            # for m in range(k):\n",
        "            #   if max_s <= successives[m][1]:\n",
        "            #     max_s = successives \n",
        "            #     M = m\n",
        "            #   else:\n",
        "            #     M = 0\n",
        "                \n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]    #一句话， 数值 取前三个\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty lost their mittens\n",
            "we naughty lost their mittens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnj1lDATn-5m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzPNre1Dn_DT"
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# To-Do: modify this function\n",
        "# Generate text, currently only works with k=1 \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "            if k == 1:\n",
        "              id, s = get_topK(predicted, k)[0]\n",
        "              output_word = ind_to_word(id)\n",
        "              successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "            else:\n",
        "              id_n =[]\n",
        "              s_n = []\n",
        "              for j in range(k):            \n",
        "                id_n.append(get_topK(predicted, k)[j][0])\n",
        "                s_n.append(get_topK(predicted, k)[j][1])\n",
        "                output_word = ind_to_word(id_n[j])   \n",
        "                successives.append((seed_text + ' ' + output_word, score - s_n[j])) \n",
        "\n",
        "         ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]   \n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI3V70BYnhfL"
      },
      "source": [
        "\n",
        "'''\n",
        "            # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "            # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "       \n",
        "            id_n =[]\n",
        "            s_n = []\n",
        "           \n",
        "          #j = 1,2 3\n",
        "            #get_topK(predicted, k)[0]  #取三个最大的【（）（）（）】list【0】第一个【1】第二个【2】第三个 id 和s 应该是list\n",
        "            id_n.append(get_topK(predicted, k)[0][0])\n",
        "            s_n.append(get_topK(predicted, k)[0][1])\n",
        "            #get_topK(predicted, 3)[0][1]    ===s  [0][0]==id\n",
        "            #id, s = get_topK(predicted, k)[k] 取出【0】第一个输入反馈个index和值\n",
        "          # get the output word\n",
        "            output_word = ind_to_word(id)   #给出个单词\n",
        "          # put the word into the sentence input\n",
        "          # calcualte the accumulated score by -log(p)\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) #给出个结果句子+分数。\n",
        "            #是把这句话添加到里面去了\n",
        "          # max_s = successives[0][1]\n",
        "          # for m in range(k):\n",
        "          #   if max_s <= successives[m][1]:\n",
        "          #     max_s = successives \n",
        "          #     M = m\n",
        "          #   else:\n",
        "          #     M = 0\n",
        "'''\n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1]) #根据分数排这句话\n",
        "        seed_candidates = ordered[:k]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b78KokvQeA46"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LEwQLPZlWJV",
        "outputId": "120bd3d4-d563-4c44-8d28-2b28137994a4"
      },
      "source": [
        "get_topK(predicted, 3)[0][1]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3.245199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YP_E6aukY4h",
        "outputId": "a49b14b5-8835-42f4-bb21-fc7e1da562e8"
      },
      "source": [
        "get_topK(predicted, 3)  #[(29, -0.23189081)]\n",
        "#[(9, -3.245199), (10, -2.3391106), (29, -0.23189081)] \n",
        "#list 里有k个数"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, -3.245199), (10, -2.3391106), (29, -0.23189081)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWm80AQojsvr",
        "outputId": "1a69d84f-b934-4458-c995-63b44a5c8606"
      },
      "source": [
        "get_topK(predicted, 0)  #[(29, -0.23189081)]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(16, -10.416989),\n",
              " (15, -10.166246),\n",
              " (34, -10.161328),\n",
              " (35, -10.045282),\n",
              " (18, -9.923558),\n",
              " (40, -9.908958),\n",
              " (30, -9.807699),\n",
              " (21, -9.688912),\n",
              " (33, -9.55206),\n",
              " (1, -9.49195),\n",
              " (42, -8.943313),\n",
              " (0, -8.87492),\n",
              " (31, -8.700661),\n",
              " (36, -8.690514),\n",
              " (2, -8.549226),\n",
              " (6, -8.16085),\n",
              " (41, -7.982132),\n",
              " (3, -7.925426),\n",
              " (22, -7.8716936),\n",
              " (27, -7.779893),\n",
              " (38, -7.6586666),\n",
              " (14, -7.6326613),\n",
              " (12, -7.499868),\n",
              " (11, -7.3750477),\n",
              " (5, -7.2503753),\n",
              " (39, -7.1362243),\n",
              " (13, -7.058437),\n",
              " (17, -6.8277965),\n",
              " (37, -6.6968994),\n",
              " (20, -6.5761285),\n",
              " (23, -6.5300627),\n",
              " (32, -6.5007334),\n",
              " (7, -6.4987125),\n",
              " (4, -6.4331303),\n",
              " (19, -6.2710686),\n",
              " (24, -5.8766117),\n",
              " (8, -4.8873186),\n",
              " (25, -4.5415936),\n",
              " (28, -4.414175),\n",
              " (26, -3.9523525),\n",
              " (9, -3.245199),\n",
              " (10, -2.3391106),\n",
              " (29, -0.23189081)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISVc6gHBjs3D",
        "outputId": "d3bee8aa-1659-46ac-8aa7-52e26f59bfcf"
      },
      "source": [
        "seed_text = \"we naughty\"\n",
        "next_words = 3\n",
        "max_sequence_len = max_sequence_len\n",
        "#k = 3 这就恒定了\n",
        "#目前是初始化，[('we naughty', 0.0)]\n",
        "seed_candidates = [(seed_text, .0)] #\"we naughty\"  [('we naughty they our kittens', 8.618810653686523)]把里面的数据取出来\n",
        "for _ in range(next_words): #3 #3  _ = 0,1,2\n",
        "    successives = [] #逐次导数？\n",
        "    # if k = 1, len(seed_candidates) will always be 1 # 如果k = 1，len(seed_candidates)将总是1\n",
        "    for i in range(len(seed_candidates)): #len(sequences) 目前里面一句话1 这里要改。\n",
        "        seed_text, score = seed_candidates[i]  #之前是文本和分数。初级然后慢慢迭代更新。\n",
        "     \n",
        "        token_list = [word_to_index[word] for word in seed_text.split()]  #[24, 41] 目前只有两个词。所以分开找we': 24,'naughty': 41,， 变成index\n",
        "        token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)   #7扩充到最大长度 list\n",
        "\n",
        "        seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)  #然后变成tensor 长度还是7 里面还是那两个单词。排在倒数的位置\n",
        "        predicted = model(seed_input).cpu().detach().numpy()  #跑了那个模型RNN变array了 变成（1，43） 这个向量维度是43，现有句子。\n",
        "\n",
        "        # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "          # 因为它只对k = 1起作用，我们可以简单地用[0]来获得单词id和log(p)\n",
        "        # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "          # 然而，如果k = 3，就不能简单地用[0]来获得候选者了\n",
        "        id, s = get_topK(predicted, 1)[0]  #把这句话的array都丢进去做gettopK了\n",
        "        # get the output word #获得输出字数\n",
        "        output_word = ind_to_word(id)\n",
        "        # put the word into the sentence input#把这个词放到句子的输入中\n",
        "        # calcualte the accumulated score by -log(p)#用-log(p)计算累计得分。 每行的第几个 的log值 \n",
        "        successives.append((seed_text + ' ' + output_word, score - s))  #candidate = [seq + [j], score + (-log(row[j])) ]、、all_candidates.append(candidate)\n",
        "  #这后面一样 #每行的第几个 的log值  #score最开始的得分\n",
        "    # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "    ## 获得最低的k个累积分数（最高的k个累积概率）。\n",
        "    # Then, make them as the seed_candidate for the next word to predict\n",
        "    # 然后，把它们作为下一个要预测的词的种子_候选者\n",
        "    ordered = sorted(successives, key=lambda tup: tup[1]) #[('we naughty go to day', 1.2721786051988602)]\n",
        "    seed_candidates = ordered[:1]     #最优的那个\n",
        "print(seed_candidates[0][0])  #we naughty go to day\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty go to day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U-ei1Wxj6OC",
        "outputId": "d50d090d-e926-44d3-de9d-9cc423eef788"
      },
      "source": [
        "seed_candidates = ordered[:1]\n",
        "seed_candidates"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we naughty go to day', 1.2721786051988602)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzLNNn4Nj4xL"
      },
      "source": [
        "ordered = sorted(successives, key=lambda tup: tup[1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YSM0O-neBAe",
        "outputId": "d692ab15-abb2-4cad-e715-468899f46f22"
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# To-Do: modify this function\n",
        "# Generate text, currently only works with k=1 \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "            # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "            # for j in range(k):\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input\n",
        "            # calcualte the accumulated score by -log(p)\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty go to pie\n",
            "we naughty their mittens ye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYf-gtT-Uiuh"
      },
      "source": [
        "**Sample Output** (Your output would be different, it is based on the trained model)\n",
        "\n",
        "\n",
        "```\n",
        "we naughty lost their mittens\n",
        "```\n",
        "\n"
      ]
    }
  ]
}