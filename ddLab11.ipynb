{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "ddLab11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/ddLab11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s55oVTgTXTDP"
      },
      "source": [
        "This code is from https://pytorch.org/tutorials/beginner/transformer_tutorial.html, pytorch official document about seq2seq modeling using Transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtb2-yhjf9C9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961de308-bb8e-4f81-e11d-d208c3cee840"
      },
      "source": [
        "!pip3 install torchtext==0.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 16.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2020.12.5)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XjsunFtWPQv"
      },
      "source": [
        "\n",
        "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on how to train a sequence-to-sequence model\n",
        "that uses the\n",
        "[nn.Transformer](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) module.\n",
        "\n",
        "PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper [Attention is All You\n",
        "Need](https://arxiv.org/pdf/1706.03762.pdf) . The transformer model\n",
        "has been proved to be superior in quality for many sequence-to-sequence\n",
        "problems while being more parallelizable. The ``nn.Transformer`` module\n",
        "relies entirely on an attention mechanism (another module recently\n",
        "implemented as [nn.MultiheadAttention](https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention) to draw global dependencies\n",
        "between input and output. The ``nn.Transformer`` module is now highly\n",
        "modularized such that a single component (like `nn.TransformerEncoder`\n",
        "in this tutorial) can be easily adapted/composed.\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/transformer_architecture.jpg?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23r1Mv0wWPQw"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K102Br-MWPQx"
      },
      "source": [
        "In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. \n",
        "\n",
        "A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "[nn.TransformerEncoderLayer](https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer) . Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
        "the sequence. For the language modeling task, any tokens on the future\n",
        "positions should be masked. To have the actual words, the output\n",
        "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
        "layer, which is followed by a log-Softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY8FTTcSa_f7"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po8nLNxXWPQy"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        #triu returns the upper triangular part of a matrix (2-D tensor) or batch of matrices (see section below)\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYv1_CYrJzFG"
      },
      "source": [
        "#### Masking\n",
        "By passing the mask into the transformer_encoder forward() function, the attention will only be calculated on the earlier positions in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKVBR--Galxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7298c19b-ba1a-414c-ba6b-42fb537b23d3"
      },
      "source": [
        "#triu returns the upper triangular part of a matrix (2-D tensor) or batch of matrices (see section below)\n",
        "torch.triu(torch.ones(3, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [0., 1., 1.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btnWrvo-ayP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df783e1-e5a4-4a60-f0d2-6fb30393fd0f"
      },
      "source": [
        "# Masking\n",
        "def masking():\n",
        "  sz = 4\n",
        "  mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "  \n",
        "  return mask\n",
        "\n",
        "masking()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh47UCwObCf-"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xdCyMPQWPQ3"
      },
      "source": [
        "Transformer architecture takes base architecture of Seq2Seq model (Encoder - Decoder). However, the transformer does not use recurrent model so this means we need a module that captures sequence information of the input/output.\n",
        "\n",
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffk_fy_eWPQ4"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) \n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65glmcV4WPQ9"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHI1s_A_WPQ-"
      },
      "source": [
        "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
        "vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26)\n",
        "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
        "length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "These columns are treated as independent by the model, which means that\n",
        "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
        "efficient batch processing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJTXDCtWPQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed83219-11ae-4fe6-f5d0-6c9ef268b2f3"
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 18.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OE4TAV9WPRD"
      },
      "source": [
        "Functions to generate input and target sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq_pEBjoWPRE"
      },
      "source": [
        "``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/transformer_input_target.png?raw=1)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPhRYAjYWPRE"
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6_5USduWPRU"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr6Wa55XWPRV"
      },
      "source": [
        "The model is set up with the hyperparameter below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6yqdrkWPRW"
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGCV6PTHWPRd"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7nw5RbXWPRf"
      },
      "source": [
        "`CrossEntropyLoss `\n",
        "is applied to track the loss and\n",
        "`SGD `\n",
        "implements stochastic gradient descent method as the optimizer. The initial\n",
        "learning rate is set to 5.0. `StepLR ` is\n",
        "applied to adjust the learn rate through epochs. During the\n",
        "training, we use\n",
        "`nn.utils.clip_grad_norm_ `\n",
        "function to scale all the gradient together to prevent exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrOT_epWPRg"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_dj-0RwWPRj"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ceh4l7p0z1c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n17HNUS4WPRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d1a2fd-5a5a-4212-fa88-3dc6f5824fb4"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 22.39 | loss  8.06 | ppl  3155.62\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 21.46 | loss  6.83 | ppl   920.94\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 21.39 | loss  6.37 | ppl   586.98\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 21.44 | loss  6.24 | ppl   511.30\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 21.39 | loss  6.12 | ppl   453.86\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 21.43 | loss  6.09 | ppl   439.98\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 21.43 | loss  6.04 | ppl   420.34\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 21.42 | loss  6.04 | ppl   420.72\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 21.51 | loss  5.96 | ppl   386.22\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 21.46 | loss  5.96 | ppl   387.19\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 21.48 | loss  5.85 | ppl   348.02\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 21.46 | loss  5.90 | ppl   363.37\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 21.42 | loss  5.90 | ppl   363.23\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 21.45 | loss  5.80 | ppl   330.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 66.55s | valid loss  5.76 | valid ppl   316.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 21.61 | loss  5.80 | ppl   330.23\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.77 | ppl   321.16\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.60 | ppl   270.22\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 21.55 | loss  5.64 | ppl   280.90\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 21.49 | loss  5.59 | ppl   267.85\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 21.47 | loss  5.62 | ppl   275.47\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 21.48 | loss  5.63 | ppl   278.82\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 21.55 | loss  5.67 | ppl   289.34\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 21.53 | loss  5.59 | ppl   267.42\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.62 | ppl   274.79\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 21.47 | loss  5.51 | ppl   246.86\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 21.48 | loss  5.57 | ppl   261.70\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 21.49 | loss  5.58 | ppl   265.82\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 21.44 | loss  5.52 | ppl   250.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 66.53s | valid loss  5.58 | valid ppl   264.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 21.73 | loss  5.54 | ppl   255.80\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 21.59 | loss  5.55 | ppl   256.76\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 21.56 | loss  5.36 | ppl   212.96\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 21.52 | loss  5.42 | ppl   225.46\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 21.58 | loss  5.37 | ppl   214.89\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 21.57 | loss  5.41 | ppl   223.92\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 21.58 | loss  5.43 | ppl   228.79\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 21.49 | loss  5.47 | ppl   238.28\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 21.46 | loss  5.41 | ppl   224.37\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 21.53 | loss  5.44 | ppl   229.65\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 21.48 | loss  5.33 | ppl   205.73\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 21.44 | loss  5.39 | ppl   220.26\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 21.42 | loss  5.41 | ppl   224.59\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 21.45 | loss  5.34 | ppl   208.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 66.56s | valid loss  5.54 | valid ppl   255.23\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stKeoW99WPRn"
      },
      "source": [
        "Evaluate the model with the test dataset\n",
        "-------------------------------------\n",
        "\n",
        "Apply the best model to check the result with the test dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxzbDUnFWPRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c46c441-c1ff-4513-d2c4-505e56545086"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.44 | test ppl   231.38\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC3ND0e8I4Z9"
      },
      "source": [
        "# Hugging Face Transformer\n",
        "\n",
        "Hugging Face is a library providing transformers and related models. The tutorial links can be found here: https://huggingface.co/transformers/notebooks.html\n",
        "\n",
        "**Note**: You MUST NOT fine-tune a pretrained model in Assignment2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4xteNApjplg"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edjjkL_SjrL0"
      },
      "source": [
        "## E1. What is an advantage of the transformer model over RNNs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjozONcUjsRm"
      },
      "source": [
        "Your Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmzzhI5iP93"
      },
      "source": [
        "## E2 Test with Transformer\n",
        "Try different number of heads in Multi-head Attention, **or** different number of encoders, and record the test performance. Draw a graph to show the test performance (or validation losses/ppls) vs number of heads, or the test performance (or validation losses/ppls) vs number of encoders, (you can keep epochs = 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L37XqRug6mKf"
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nheads = [1,2,4,8] # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "Model =[]\n",
        "for nhead in nheads:\n",
        "  model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "  Model.append(model)\n",
        "Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10V5mZI94w_y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBBC3_Ep2PGu",
        "outputId": "75b4dc73-1817-4b72-8d43-c45314842ff5"
      },
      "source": [
        "performance = []\n",
        "T_performance = []\n",
        "for I in range(len(Model)):\n",
        "  print('''\n",
        "====================================================================================================\n",
        "Training the model with following configs: number of heads=%d & number of encoders=2\n",
        "====================================================================================================\n",
        "''' %(nheads[I]))\n",
        "  model = Model[I]\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  lr = 5.0 # learning rate\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "  best_val_loss = float(\"inf\")\n",
        "  epochs = 3 # The number of epochs\n",
        "  best_model = None\n",
        "\n",
        "  perf = []\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      epoch_start_time = time.time()\n",
        "      train()\n",
        "      val_loss = evaluate(model, val_data)\n",
        "      print('-' * 89)\n",
        "      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                      val_loss, math.exp(val_loss)))\n",
        "      print('-' * 89)\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          best_model = model\n",
        "\n",
        "      perf.append(val_loss)\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "  test_loss = evaluate(best_model, test_data)\n",
        "  print('=' * 89)\n",
        "  print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "      test_loss, math.exp(test_loss)))\n",
        "  print('=' * 89)\n",
        "\n",
        "  performance.append(perf)\n",
        "  T_performance.append(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "Training the model with following configs: number of heads=1 & number of encoders=2\n",
            "====================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 21.77 | loss  5.07 | ppl   159.00\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 21.39 | loss  5.09 | ppl   162.01\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 21.37 | loss  4.90 | ppl   133.94\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 21.37 | loss  5.01 | ppl   150.21\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 21.43 | loss  5.08 | ppl   160.78\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 21.46 | loss  5.12 | ppl   166.52\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 21.43 | loss  5.16 | ppl   175.01\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 21.42 | loss  5.20 | ppl   181.25\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 21.48 | loss  5.15 | ppl   172.48\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 21.43 | loss  5.15 | ppl   173.27\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 21.42 | loss  5.03 | ppl   153.41\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 21.39 | loss  5.11 | ppl   165.79\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 21.41 | loss  5.14 | ppl   170.75\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 21.40 | loss  5.09 | ppl   161.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 66.33s | valid loss  5.45 | valid ppl   232.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.01 | ppl   149.98\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 21.44 | loss  5.01 | ppl   149.16\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 21.37 | loss  4.83 | ppl   124.61\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 21.35 | loss  4.93 | ppl   138.98\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 21.46 | loss  4.98 | ppl   145.98\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 21.39 | loss  5.03 | ppl   153.56\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 21.46 | loss  5.07 | ppl   158.51\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 21.40 | loss  5.11 | ppl   165.56\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 21.39 | loss  5.06 | ppl   158.27\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 21.39 | loss  5.08 | ppl   160.23\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 21.43 | loss  4.96 | ppl   142.03\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 21.44 | loss  5.03 | ppl   153.09\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 21.41 | loss  5.07 | ppl   158.44\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 21.47 | loss  5.00 | ppl   148.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 66.27s | valid loss  5.51 | valid ppl   248.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 21.53 | loss  4.94 | ppl   140.03\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 21.39 | loss  4.94 | ppl   139.48\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 21.36 | loss  4.76 | ppl   116.27\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 21.38 | loss  4.85 | ppl   127.46\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 21.37 | loss  4.92 | ppl   137.26\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 21.39 | loss  4.95 | ppl   140.99\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 21.37 | loss  4.98 | ppl   145.72\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 21.37 | loss  5.02 | ppl   151.52\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 21.41 | loss  5.01 | ppl   150.65\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 21.43 | loss  4.99 | ppl   147.12\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 21.41 | loss  4.87 | ppl   130.33\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 21.49 | loss  4.95 | ppl   141.19\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 21.43 | loss  5.00 | ppl   148.02\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 21.50 | loss  4.92 | ppl   137.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 66.25s | valid loss  5.54 | valid ppl   253.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.43 | test ppl   228.96\n",
            "=========================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Training the model with following configs: number of heads=2 & number of encoders=2\n",
            "====================================================================================================\n",
            "\n",
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 21.70 | loss  5.42 | ppl   225.11\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 21.61 | loss  5.45 | ppl   231.80\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 21.59 | loss  5.27 | ppl   193.47\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 21.51 | loss  5.31 | ppl   202.46\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 21.61 | loss  5.27 | ppl   193.91\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 21.51 | loss  5.31 | ppl   203.07\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 21.50 | loss  5.33 | ppl   207.38\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 21.50 | loss  5.38 | ppl   217.60\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 21.51 | loss  5.32 | ppl   204.87\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 21.50 | loss  5.36 | ppl   212.01\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 21.46 | loss  5.23 | ppl   187.18\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 21.46 | loss  5.32 | ppl   204.89\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 21.48 | loss  5.34 | ppl   207.49\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 21.49 | loss  5.25 | ppl   190.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 66.58s | valid loss  5.58 | valid ppl   265.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 21.62 | loss  5.30 | ppl   200.45\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 21.55 | loss  5.33 | ppl   205.44\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 21.55 | loss  5.13 | ppl   169.28\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 21.60 | loss  5.19 | ppl   180.11\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 21.54 | loss  5.16 | ppl   174.60\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.20 | ppl   182.07\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.23 | ppl   187.37\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 21.57 | loss  5.28 | ppl   196.56\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 21.62 | loss  5.23 | ppl   186.97\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 21.48 | loss  5.25 | ppl   190.26\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 21.56 | loss  5.11 | ppl   166.39\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 21.53 | loss  5.20 | ppl   181.48\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 21.48 | loss  5.23 | ppl   186.09\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.16 | ppl   174.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 66.62s | valid loss  5.50 | valid ppl   243.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 21.56 | loss  5.19 | ppl   179.42\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 21.49 | loss  5.23 | ppl   187.36\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 21.59 | loss  5.03 | ppl   153.61\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 21.59 | loss  5.12 | ppl   167.81\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 21.56 | loss  5.07 | ppl   159.07\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 21.49 | loss  5.11 | ppl   166.28\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 21.48 | loss  5.15 | ppl   171.96\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 21.52 | loss  5.19 | ppl   179.72\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 21.54 | loss  5.13 | ppl   169.48\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 21.58 | loss  5.17 | ppl   176.41\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 21.48 | loss  5.04 | ppl   154.48\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 21.57 | loss  5.12 | ppl   167.51\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 21.47 | loss  5.13 | ppl   169.63\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 21.49 | loss  5.09 | ppl   163.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 66.57s | valid loss  5.48 | valid ppl   240.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.39 | test ppl   218.45\n",
            "=========================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Training the model with following configs: number of heads=4 & number of encoders=2\n",
            "====================================================================================================\n",
            "\n",
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 21.63 | loss  5.44 | ppl   230.06\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 21.48 | loss  5.46 | ppl   235.53\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 21.48 | loss  5.27 | ppl   193.47\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 21.51 | loss  5.35 | ppl   209.89\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 21.59 | loss  5.29 | ppl   198.62\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 21.58 | loss  5.33 | ppl   206.43\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 21.63 | loss  5.35 | ppl   211.50\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 21.54 | loss  5.42 | ppl   225.59\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 21.51 | loss  5.34 | ppl   208.21\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 21.56 | loss  5.39 | ppl   218.31\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 21.60 | loss  5.26 | ppl   191.76\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 21.60 | loss  5.33 | ppl   207.09\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 21.65 | loss  5.35 | ppl   210.10\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 21.54 | loss  5.29 | ppl   197.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 66.69s | valid loss  5.47 | valid ppl   236.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 21.59 | loss  5.34 | ppl   207.78\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 21.43 | loss  5.35 | ppl   210.33\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 21.46 | loss  5.16 | ppl   174.58\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 21.46 | loss  5.23 | ppl   187.57\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 21.47 | loss  5.20 | ppl   181.15\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 21.53 | loss  5.24 | ppl   188.29\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 21.59 | loss  5.26 | ppl   192.60\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 21.50 | loss  5.32 | ppl   203.95\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 21.58 | loss  5.26 | ppl   191.60\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 21.58 | loss  5.28 | ppl   196.36\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 21.51 | loss  5.15 | ppl   172.54\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 21.58 | loss  5.23 | ppl   186.67\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 21.54 | loss  5.25 | ppl   191.10\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 21.63 | loss  5.19 | ppl   179.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 66.59s | valid loss  5.45 | valid ppl   231.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 21.60 | loss  5.23 | ppl   186.32\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 21.50 | loss  5.26 | ppl   192.01\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 21.43 | loss  5.07 | ppl   159.01\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 21.47 | loss  5.15 | ppl   172.12\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 21.43 | loss  5.11 | ppl   165.99\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 21.50 | loss  5.15 | ppl   172.75\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 21.53 | loss  5.16 | ppl   174.89\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 21.55 | loss  5.22 | ppl   185.59\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 21.59 | loss  5.17 | ppl   175.33\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 21.55 | loss  5.22 | ppl   184.57\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 21.51 | loss  5.07 | ppl   159.95\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 21.49 | loss  5.16 | ppl   173.67\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 21.59 | loss  5.17 | ppl   176.51\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 21.55 | loss  5.11 | ppl   166.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 66.56s | valid loss  5.46 | valid ppl   235.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.36 | test ppl   213.42\n",
            "=========================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Training the model with following configs: number of heads=8 & number of encoders=2\n",
            "====================================================================================================\n",
            "\n",
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 21.82 | loss  5.45 | ppl   232.59\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 21.76 | loss  5.47 | ppl   237.14\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 21.71 | loss  5.27 | ppl   194.44\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 21.66 | loss  5.33 | ppl   206.81\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 21.70 | loss  5.30 | ppl   200.52\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 21.68 | loss  5.37 | ppl   214.24\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 21.68 | loss  5.38 | ppl   216.84\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 21.72 | loss  5.43 | ppl   227.39\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 21.68 | loss  5.35 | ppl   210.15\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 21.66 | loss  5.39 | ppl   218.63\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 21.62 | loss  5.27 | ppl   193.97\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 21.64 | loss  5.36 | ppl   213.06\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 21.67 | loss  5.37 | ppl   215.40\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 21.66 | loss  5.31 | ppl   203.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 67.04s | valid loss  5.58 | valid ppl   265.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 21.85 | loss  5.34 | ppl   209.31\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 21.66 | loss  5.37 | ppl   214.55\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 21.62 | loss  5.19 | ppl   178.85\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 21.65 | loss  5.25 | ppl   191.16\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 21.65 | loss  5.21 | ppl   182.55\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 21.74 | loss  5.25 | ppl   191.51\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 21.70 | loss  5.28 | ppl   197.13\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 21.72 | loss  5.34 | ppl   207.65\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 21.75 | loss  5.27 | ppl   195.00\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 21.63 | loss  5.31 | ppl   201.91\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 21.62 | loss  5.18 | ppl   178.26\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 21.62 | loss  5.27 | ppl   194.54\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 21.62 | loss  5.29 | ppl   198.05\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 21.67 | loss  5.23 | ppl   187.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 67.03s | valid loss  5.48 | valid ppl   238.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 21.88 | loss  5.26 | ppl   192.55\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 21.71 | loss  5.30 | ppl   200.17\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 21.70 | loss  5.10 | ppl   163.88\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 21.74 | loss  5.16 | ppl   174.90\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 21.65 | loss  5.14 | ppl   170.82\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 21.67 | loss  5.18 | ppl   177.64\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 21.66 | loss  5.21 | ppl   182.45\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 21.71 | loss  5.25 | ppl   191.41\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 21.73 | loss  5.20 | ppl   181.65\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 21.73 | loss  5.23 | ppl   186.66\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 21.72 | loss  5.12 | ppl   166.52\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 21.66 | loss  5.21 | ppl   182.30\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 21.65 | loss  5.21 | ppl   182.55\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 21.64 | loss  5.15 | ppl   173.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 67.07s | valid loss  5.42 | valid ppl   226.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.33 | test ppl   206.26\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Bd_jByAGIK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJya3UQo7VDm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "pyX-hEG1U6m6",
        "outputId": "caff7ca8-3ce2-4fd8-ce55-9adfd5350457"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel(\"number of heads\")\n",
        "plt.ylabel(\"test performance\")\n",
        "plt.title(\"Test performance - Number of heads\")\n",
        "\n",
        "plt.bar(nheads,T_performance, color = 'b')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"performance\")\n",
        "plt.title(\"performance - Number of heads\")\n",
        "\n",
        "epoch = [1,2,3]\n",
        "plt.plot(epoch,performance[0],'blue',markersize = 5, label = '1 heads')\n",
        "plt.plot(epoch,performance[1],'orange',markersize = 5, label = '2 heads')\n",
        "plt.plot(epoch,performance[2],'green',markersize = 5, label = '4 heads')\n",
        "plt.plot(epoch,performance[3],'red',markersize = 5, label = '8 heads')\n",
        "plt.legend(nheads)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oml50dl8G6X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzOECAJVC2X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}