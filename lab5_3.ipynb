{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5.3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+pGlwa04+bMBWVXxoaOI6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/lab5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUCUC6Q6lnf1"
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7lyuAzZlsph"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1gNfBqguzBu8cHKMPc8C44GbvD443dNC5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('twitter.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"twitter.csv\")\n",
        "df_pick = df.sample(400,random_state=24)\n",
        "\n",
        "raw_text = df_pick[\"Text\"].tolist()\n",
        "raw_label = df_pick[\"Label\"].tolist()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train,text_test,label_train,label_test = train_test_split(raw_text,raw_label,test_size=0.25,random_state=42) #test_size=0.25表示取了四分之一的数据来测试模型"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_UChL7lwlN"
      },
      "source": [
        "text_train = [s.lower() for s in text_train]\n",
        "text_test = [s.lower() for s in text_test]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTC2hIXyl4Mm"
      },
      "source": [
        "import re\n",
        "def remove_punctuation_re(x):\n",
        "    # Please complete this\n",
        "    x = re.sub(r'[^\\w\\s]','',str(x))  \n",
        "    return x\n",
        "    \n",
        "text_train = [remove_punctuation_re(s) for s in text_train]\n",
        "text_test = [remove_punctuation_re(s) for s in text_test]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2k4AgF6mRHT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL_0B4NzmD0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7066420-c6a9-4828-dd30-24ead858ac15"
      },
      "source": [
        "#token\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "#Please complete this\n",
        "\n",
        "text_train = word_tokenize(str(sent_tokenize(str(text_train))))\n",
        "text_test = word_tokenize(str(sent_tokenize(str(text_test))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0yhM_Wtmxsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e10e53-8415-4e1b-fc2b-1c7fdd44f0b3"
      },
      "source": [
        "#remove\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "\n",
        "text_train_ns=[]\n",
        "for tokens in text_train:\n",
        "  filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "  text_train_ns.append(filtered_sentence)\n",
        "\n",
        "text_test_ns=[]\n",
        "for tokens in text_test:\n",
        "  filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "  text_train_ns.append(filtered_sentence)\n",
        "    #Please complete this"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX79itRsmEEG"
      },
      "source": [
        "Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHjLPyhLnGMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d297feed-28f3-4f19-812d-e9a39da1aaea"
      },
      "source": [
        "text_train_ns[10:20]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['r'], [\"'\"], [','], [\"'\"], [], [], ['1', '7'], ['r'], ['r'], ['x', 'p']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-kFsjVmFX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ec2150-5fe6-4696-c4c6-a6998ae52406"
      },
      "source": [
        "text_train[10:20]#,"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['matter',\n",
              " \"'\",\n",
              " ',',\n",
              " \"'sevilzadeh\",\n",
              " 'mohammed',\n",
              " 'led',\n",
              " '17',\n",
              " 'major',\n",
              " 'military',\n",
              " 'expeditions']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skvtfje-mIBf"
      },
      "source": [
        "type(text_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmMjdP7moA3"
      },
      "source": [
        "171     def addSentence(self, sentence):\n",
        "172         for word in sentence.split(' '):\n",
        "173             self.addWord(word)  # 用于添加单词\n",
        "174 \n",
        "175     def addWord(self, word):\n",
        "176         if word not in self.word2index:  # 是不是新的词\n",
        "177             # 如果不在word2index里，则需要新的定义字典\n",
        "178             self.word2index[word] = self.n_words\n",
        "179             self.word2count[word] = 1\n",
        "180             self.index2word[self.n_words] = word\n",
        "181             self.n_words += 1  # 相当于每次index+1\n",
        "182         else:\n",
        "183             self.word2count[word] += 1  # 计算每次词的个数\n",
        "2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}