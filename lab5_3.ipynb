{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5.3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCXV/8IwT/X4hDFhEJza/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/lab5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUCUC6Q6lnf1"
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7lyuAzZlsph"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1gNfBqguzBu8cHKMPc8C44GbvD443dNC5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('twitter.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"twitter.csv\")\n",
        "df_pick = df.sample(400,random_state=24)\n",
        "\n",
        "raw_text = df_pick[\"Text\"].tolist()\n",
        "raw_label = df_pick[\"Label\"].tolist()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train,text_test,label_train,label_test = train_test_split(raw_text,raw_label,test_size=0.25,random_state=42) #test_size=0.25表示取了四分之一的数据来测试模型"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_UChL7lwlN"
      },
      "source": [
        "text_train = [s.lower() for s in text_train]\n",
        "text_test = [s.lower() for s in text_test]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTC2hIXyl4Mm"
      },
      "source": [
        "import re\n",
        "def remove_punctuation_re(x):\n",
        "    # Please complete this\n",
        "    x = re.sub(r'[^\\w\\s]','',str(x))  \n",
        "    return x\n",
        "    \n",
        "text_train = [remove_punctuation_re(s) for s in text_train]\n",
        "text_test = [remove_punctuation_re(s) for s in text_test]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2k4AgF6mRHT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL_0B4NzmD0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b9994a-2440-4aa3-b66f-dd9343cc59eb"
      },
      "source": [
        "#token\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "#Please complete this\n",
        "\n",
        "text_train = word_tokenize(str(sent_tokenize(str(text_train))))\n",
        "text_test = word_tokenize(str(sent_tokenize(str(text_test))))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0yhM_Wtmxsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc7deef-0be8-49d5-dfb3-201e5edb0207"
      },
      "source": [
        "#remove\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "\n",
        "text_train_ns=[]\n",
        "for tokens in text_train:\n",
        "  filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "  text_train_ns.append(filtered_sentence)\n",
        "\n",
        "text_test_ns=[]\n",
        "for tokens in text_test:\n",
        "  filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "  text_train_ns.append(filtered_sentence)\n",
        "    #Please complete this"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX79itRsmEEG"
      },
      "source": [
        "Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHjLPyhLnGMx"
      },
      "source": [
        "text_train_ns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-kFsjVmFX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb7e299-1324-4706-a40c-f029967a03d0"
      },
      "source": [
        "text_train[10:20]#,"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',',\n",
              " \"'sevilzadeh\",\n",
              " 'mohammed',\n",
              " 'led',\n",
              " '17',\n",
              " 'major',\n",
              " 'military',\n",
              " 'expeditions',\n",
              " 'most',\n",
              " 'offensive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skvtfje-mIBf"
      },
      "source": [
        "type(text_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmMjdP7moA3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}