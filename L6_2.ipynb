{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L6.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkWTDvypKrx4EwP/VZmj5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/L6_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ZM7LLTTRXO"
      },
      "source": [
        "dHMM 什么是隐形：part of speech，什么是显性：单词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHXmDjxQg_y0",
        "outputId": "3c59fc4d-43b1-4445-cf8d-dcfd71e4ecf5"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "import nltk\n",
        "\n",
        "# Downloading required corpus\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7V6CH7PhGEF"
      },
      "source": [
        "\n",
        "brown_tags_words = []\n",
        "brown_tagged_sents = brown.tagged_sents()\n",
        "\n",
        "for sent in brown_tagged_sents:\n",
        "    # sent is a list of word/tag pairs\n",
        "    # add START/START at the beginning\n",
        "    brown_tags_words.append( (\"START\", \"START\") )\n",
        "    \n",
        "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
        "    # shorten tags to 3 characters each\n",
        "    brown_tags_words.extend([ (tag[:3], word) for (word, tag) in sent ])\n",
        "    # then END/END\n",
        "    brown_tags_words.append( (\"END\", \"END\") )\n",
        "\n",
        "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words) \n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
        "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
        "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
        "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WijoHCVzhLD-",
        "outputId": "eaa3a516-0739-4536-9887-da113b9b3853"
      },
      "source": [
        "#要的***\n",
        "\n",
        "distinct_tags = set(brown_tags) #词性库\n",
        "sentence =[\n",
        "    word_tokenize(\"This race is awesome, I want to race too.\"),\n",
        "    word_tokenize(\"That race is silly, I do not want to race.\")\n",
        "]\n",
        "sentlen = len(sentence)\n",
        "def test_model(test_samples):\n",
        "  token_sequences = []\n",
        "  for l in range(sentlen):\n",
        "    token = []\n",
        "    viterbi = [ ]\n",
        "    backpointer = [ ]\n",
        "\n",
        "    first_viterbi = { }\n",
        "    first_backpointer = { }\n",
        "    for tag in distinct_tags:\n",
        "\n",
        "        if tag == \"START\": continue\n",
        "        first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[l][0] )\n",
        "        first_backpointer[ tag ] = \"START\"\n",
        "        \n",
        "    viterbi.append(first_viterbi)\n",
        "    backpointer.append(first_backpointer)\n",
        "\n",
        "    currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
        "\n",
        "    for wordindex in range(0, len(sentence[l])):\n",
        "      this_viterbi = { }\n",
        "      this_backpointer = { }\n",
        "      prev_viterbi = viterbi[-1]\n",
        "\n",
        "\n",
        "      for tag in distinct_tags:\n",
        "          if tag == \"START\": continue\n",
        "          for word in sentence[l]:  # P[Y]*P(X|Y)*P(w|X)\n",
        "            best_previous = max(prev_viterbi.keys(),\n",
        "                                key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[l][wordindex]))\n",
        "            this_viterbi[ tag ] = prev_viterbi[ best_previous] * cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[l][wordindex])\n",
        "            this_backpointer[ tag ] = best_previous\n",
        "      currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
        "      token.append(currbest)\n",
        "      viterbi.append(this_viterbi)\n",
        "      backpointer.append(this_backpointer)\n",
        "    token_sequences.append(token)\n",
        "  return token_sequences\n",
        "print(sentence)\n",
        "print(test_model(sentence))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['This', 'race', 'is', 'awesome', ',', 'I', 'want', 'to', 'race', 'too', '.'], ['That', 'race', 'is', 'silly', ',', 'I', 'do', 'not', 'want', 'to', 'race', '.']]\n",
            "[['DT', 'NN', 'BEZ', 'JJ', ',', 'PPS', 'VB', 'TO', 'VB', 'QL', '.'], ['DT', 'NN', 'BEZ', 'JJ', ',', 'PPS', 'DO', '*', 'VB', 'TO', 'VB', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5sVH1tT2P8e"
      },
      "source": [
        "# DT, NN, VBZ(BEZ),NN(JJ), , ,  PRP(PPS),VBP(VB),TO,VB,RB(OL),.; DT,NN,VBZ(BEZ),JJ, , , PRP(PPS), VBP(DO), RB(*),VB,TO,VB,./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhX_PR4EXPlJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}