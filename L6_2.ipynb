{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L6.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiwGE2VAmjBbdwuYGK/2xb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/L6_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHXmDjxQg_y0",
        "outputId": "2e0af7c0-fc90-44f5-a898-ef78c3d58f86"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "import nltk\n",
        "\n",
        "# Downloading required corpus\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "import pprint\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_sents = brown.sents(categories='news')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7V6CH7PhGEF",
        "outputId": "12e69b58-b152-4639-f330-6143d069dbe2"
      },
      "source": [
        "\n",
        "brown_tags_words = []\n",
        "brown_tagged_sents = brown.tagged_sents()\n",
        "\n",
        "for sent in brown_tagged_sents:\n",
        "    # sent is a list of word/tag pairs\n",
        "    # add START/START at the beginning\n",
        "    brown_tags_words.append( (\"START\", \"START\") )\n",
        "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
        "    # shorten tags to 2 characters each\n",
        "    brown_tags_words.extend([ (tag[:2], word) for (word, tag) in sent ])\n",
        "    # then END/END\n",
        "    brown_tags_words.append( (\"END\", \"END\") )\n",
        "\n",
        "# conditional frequency distribution\n",
        "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
        "# conditional probability distribution\n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
        "\n",
        "print(\"The probability of an adjective (JJ) being 'new' is\", cpd_tagwords[\"JJ\"].prob(\"new\"))\n",
        "print(\"The probability of a verb (VB) being 'duck' is\", cpd_tagwords[\"VB\"].prob(\"duck\"))\n",
        "\n",
        "# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n",
        "# P(ti | t{i-1}) = count(t{i-1}, ti) / count(t{i-1})\n",
        "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
        "\n",
        "# make conditional frequency distribution:\n",
        "# count(t{i-1} ti)\n",
        "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
        "# make conditional probability distribution, using\n",
        "# maximum likelihood estimate:\n",
        "# P(ti | t{i-1})\n",
        "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
        "\n",
        "print(\"If we have just seen 'DT', the probability of 'NN' is\", cpd_tags[\"DT\"].prob(\"NN\"))\n",
        "print( \"If we have just seen 'VB', the probability of 'JJ' is\", cpd_tags[\"VB\"].prob(\"DT\"))\n",
        "print( \"If we have just seen 'VB', the probability of 'NN' is\", cpd_tags[\"VB\"].prob(\"NN\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The probability of an adjective (JJ) being 'new' is 0.01472344917632025\n",
            "The probability of a verb (VB) being 'duck' is 6.042713350943527e-05\n",
            "If we have just seen 'DT', the probability of 'NN' is 0.5057722522030194\n",
            "If we have just seen 'VB', the probability of 'JJ' is 0.016885067592065053\n",
            "If we have just seen 'VB', the probability of 'NN' is 0.10970977711020183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhEG1ulniJhb",
        "outputId": "6fb2a6e8-6de2-4cd3-a40e-86358fdc347c"
      },
      "source": [
        "len(sentence[l])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WijoHCVzhLD-",
        "outputId": "1db699a7-b7e5-49f4-863a-af617f347c30"
      },
      "source": [
        "#要的***\n",
        "\n",
        "distinct_tags = set(brown_tags) #词性库\n",
        "sentence =[\n",
        "    word_tokenize(\"This race is awesome, I want to race too.\"),\n",
        "    word_tokenize(\"That race is silly, I do not want to race.\")\n",
        "]\n",
        "sentlen = len(sentence)\n",
        "def test_model(test_samples):\n",
        "  token_sequences = []\n",
        "  for l in range(sentlen):\n",
        "    token = []\n",
        "    viterbi = [ ]\n",
        "    token = []\n",
        "    backpointer = [ ]\n",
        "\n",
        "    first_viterbi = { }\n",
        "    first_backpointer = { }\n",
        "    for tag in distinct_tags:\n",
        "\n",
        "        if tag == \"START\": continue\n",
        "        first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[l][0] )\n",
        "        first_backpointer[ tag ] = \"START\"\n",
        "        \n",
        "    viterbi.append(first_viterbi)\n",
        "    backpointer.append(first_backpointer)\n",
        "\n",
        "    currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
        "\n",
        "    for wordindex in range(0, len(sentence[l])):\n",
        "      this_viterbi = { }\n",
        "      this_backpointer = { }\n",
        "      prev_viterbi = viterbi[-1]\n",
        "\n",
        "\n",
        "      for tag in distinct_tags:\n",
        "          if tag == \"START\": continue\n",
        "          for word in sentence[l]:\n",
        "            best_previous = max(prev_viterbi.keys(),\n",
        "                                key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[l][wordindex]))\n",
        "            this_viterbi[ tag ] = prev_viterbi[ best_previous] * cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[l][wordindex])\n",
        "            this_backpointer[ tag ] = best_previous\n",
        "      currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
        "      token.append(currbest)\n",
        "      viterbi.append(this_viterbi)\n",
        "      backpointer.append(this_backpointer)\n",
        "    token_sequences.append(token)\n",
        "  return token_sequences\n",
        "print(sentence)\n",
        "print(test_model(sentence))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['This', 'race', 'is', 'awesome', ',', 'I', 'want', 'to', 'race', 'too', '.'], ['That', 'race', 'is', 'silly', ',', 'I', 'do', 'not', 'want', 'to', 'race', '.']]\n",
            "[['DT', 'NN', 'BE', 'JJ', ',', 'PP', 'VB', 'TO', 'NN', 'QL', '.'], ['DT', 'NN', 'BE', 'JJ', ',', 'PP', 'DO', '*', 'VB', 'TO', 'NN', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0d6JVd0hO4I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}