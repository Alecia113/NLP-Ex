{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L6.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwdV6LUzBcetTcc51xhhDb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/L6_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHXmDjxQg_y0",
        "outputId": "01c64426-c5bd-420f-d4fc-51b194e94b46"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "import nltk\n",
        "\n",
        "# Downloading required corpus\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "import pprint\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "brown_sents = brown.sents(categories='news')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7V6CH7PhGEF",
        "outputId": "792bdf54-71fc-4eee-b368-46b86e54d7a8"
      },
      "source": [
        "\n",
        "brown_tags_words = []\n",
        "brown_tagged_sents = brown.tagged_sents()\n",
        "\n",
        "for sent in brown_tagged_sents:\n",
        "    # sent is a list of word/tag pairs\n",
        "    # add START/START at the beginning\n",
        "    brown_tags_words.append( (\"START\", \"START\") )\n",
        "    \n",
        "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
        "    # shorten tags to 3 characters each\n",
        "    brown_tags_words.extend([ (tag[:3], word) for (word, tag) in sent ])\n",
        "    # then END/END\n",
        "    brown_tags_words.append( (\"END\", \"END\") )\n",
        "\n",
        "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words) \n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
        "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
        "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
        "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ConditionalProbDist with 146 conditions>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WijoHCVzhLD-",
        "outputId": "ca0d30e7-63ee-4474-aa85-3bc37c174df8"
      },
      "source": [
        "#要的***\n",
        "\n",
        "distinct_tags = set(brown_tags) #词性库\n",
        "sentence =[\n",
        "    word_tokenize(\"This race is awesome, I want to race too.\"),\n",
        "    word_tokenize(\"That race is silly, I do not want to race.\")\n",
        "]\n",
        "sentlen = len(sentence)\n",
        "def test_model(test_samples):\n",
        "  token_sequences = []\n",
        "  for l in range(sentlen):\n",
        "    token = []\n",
        "    viterbi = [ ]\n",
        "    token = []\n",
        "    backpointer = [ ]\n",
        "\n",
        "    first_viterbi = { }\n",
        "    first_backpointer = { }\n",
        "    for tag in distinct_tags:\n",
        "\n",
        "        if tag == \"START\": continue\n",
        "        first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[l][0] )\n",
        "        first_backpointer[ tag ] = \"START\"\n",
        "        \n",
        "    viterbi.append(first_viterbi)\n",
        "    backpointer.append(first_backpointer)\n",
        "\n",
        "    currbest = max(first_viterbi.keys(), key = lambda tag: first_viterbi[ tag ])\n",
        "\n",
        "    for wordindex in range(0, len(sentence[l])):\n",
        "      this_viterbi = { }\n",
        "      this_backpointer = { }\n",
        "      prev_viterbi = viterbi[-1]\n",
        "\n",
        "\n",
        "      for tag in distinct_tags:\n",
        "          if tag == \"START\": continue\n",
        "          for word in sentence[l]:  # P[Y]*P(X|Y)*P(w|X)\n",
        "            best_previous = max(prev_viterbi.keys(),\n",
        "                                key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[l][wordindex]))\n",
        "            this_viterbi[ tag ] = prev_viterbi[ best_previous] * cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[l][wordindex])\n",
        "            this_backpointer[ tag ] = best_previous\n",
        "      currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
        "      token.append(currbest)\n",
        "      viterbi.append(this_viterbi)\n",
        "      backpointer.append(this_backpointer)\n",
        "    token_sequences.append(token)\n",
        "  return token_sequences\n",
        "print(sentence)\n",
        "print(test_model(sentence))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['This', 'race', 'is', 'awesome', ',', 'I', 'want', 'to', 'race', 'too', '.'], ['That', 'race', 'is', 'silly', ',', 'I', 'do', 'not', 'want', 'to', 'race', '.']]\n",
            "[['DT', 'NN', 'BEZ', 'JJ', ',', 'PPS', 'VB', 'TO', 'VB', 'QL', '.'], ['DT', 'NN', 'BEZ', 'JJ', ',', 'PPS', 'DO', '*', 'VB', 'TO', 'VB', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0d6JVd0hO4I"
      },
      "source": [
        "\n",
        "brown_tags_words = []\n",
        "brown_tagged_sents = brown.tagged_sents()\n",
        "\n",
        "for sent in brown_tagged_sents:\n",
        "    # sent is a list of word/tag pairs\n",
        "    # add START/START at the beginning\n",
        "    brown_tags_words.append( (\"START\", \"START\") )\n",
        "    # then all the tag/word pairs for the word/tag pairs in the sentence.\n",
        "    # shorten tags to 2 characters each\n",
        "    brown_tags_words.extend([ (tag[:2], word) for (word, tag) in sent ])\n",
        "    # then END/END\n",
        "    brown_tags_words.append( (\"END\", \"END\") )\n",
        "\n",
        "# conditional frequency distribution\n",
        "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
        "# conditional probability distribution\n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
        "\n",
        "print(\"The probability of an adjective (JJ) being 'new' is\", cpd_tagwords[\"JJ\"].prob(\"new\"))\n",
        "print(\"The probability of a verb (VB) being 'duck' is\", cpd_tagwords[\"VB\"].prob(\"duck\"))\n",
        "\n",
        "# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n",
        "# P(ti | t{i-1}) = count(t{i-1}, ti) / count(t{i-1})\n",
        "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
        "\n",
        "# make conditional frequency distribution:\n",
        "# count(t{i-1} ti)\n",
        "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
        "# make conditional probability distribution, using\n",
        "# maximum likelihood estimate:\n",
        "# P(ti | t{i-1})\n",
        "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
        "\n",
        "\n",
        "print(\"If we have just seen 'DT', the probability of 'NN' is\", cpd_tags[\"DT\"].prob(\"NN\"))\n",
        "print( \"If we have just seen 'VB', the probability of 'JJ' is\", cpd_tags[\"VB\"].prob(\"DT\"))\n",
        "print( \"If we have just seen 'VB', the probability of 'NN' is\", cpd_tags[\"VB\"].prob(\"NN\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}