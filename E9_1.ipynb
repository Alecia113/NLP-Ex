{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E9.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxBEyC7Pv64wOHaXEpDG+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/E9_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQbtE0AYpvtX"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RTKeIuVb7-l"
      },
      "source": [
        "## E1. Explain the difference between coreference and anaphora with the example. (Your answer will not be marked if you do not add the example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf0yB6zn743s"
      },
      "source": [
        " 用**例子**解释**核心推理**和**拟人**之间的**区别**。如果你不添加例子，你的答案将不会被评分\n",
        " coreference ————anaphora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3LY2lrab9Rh"
      },
      "source": [
        "Your answer: coreference： 代指； Hospital ……it ； anaphora：重复， Hospital…… hospital……"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKEtDEnBbihp"
      },
      "source": [
        "## E2. Try Bi-LSTM with CRF!\n",
        "\n",
        "Now we will apply the Bi-LSTM CRF model we just learned to CoNLL 2003 NER dataset using the pretrained glove embeddings. Please go through and complete the [Function for accuracy] section. \n",
        "现在我们将把刚刚学到的Bi-LSTM CRF模型应用于CoNLL 2003 NER数据集，使用预训练的手套嵌入。请浏览并完成[准确性的功能]部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6QUEMm70NZI"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBdXq695pyuS"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egNMhYx7qFs7"
      },
      "source": [
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"train.txt\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"train.txt\", \"1UmNHdUZxjfcuIzCcAKuBvfBXdSWFv47i\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"validation.txt\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"validation.txt\", \"11bZIh5V9m2nZJ5s5xQ_gxHEHkAEhV8eQ\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"test.txt\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"test.txt\", \"1V-LQuJWT62aCytYuhZuaxvICsqiF1rdK\"]])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsURxyItqNCT"
      },
      "source": [
        "def read_data(file_name, n_sample):\n",
        "    f = open(file_name)\n",
        "    documents = f.readlines()\n",
        "\n",
        "    input_data = []\n",
        "    target_data = []\n",
        "\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for i in documents:\n",
        "        if i == '\\n':\n",
        "            input_data.append(temp1)\n",
        "            target_data.append(temp2)\n",
        "            temp1 = []\n",
        "            temp2 = []\n",
        "        else:\n",
        "            temp1.append(i.replace('\\n','').split(' ')[0].lower())\n",
        "            temp2.append(i.replace('\\n','').split(' ')[3])\n",
        "    return input_data[:n_sample], target_data[:n_sample]\n",
        "\n",
        "train_data, target_y_train = read_data(\"train.txt\",400)   # ['york', ',', 'england', '1996-08-22']]一维一句话，二维一个token \n",
        "# ['I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],400 ,['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']一句话的标签；看起来像IO没看到B\n",
        "validation_data, target_y_validation = read_data(\"validation.txt\",50) #验证['london', '1996-08-30'],['I-LOC', 'O']\n",
        "test_data, target_y_test = read_data(\"test.txt\",50) #['nadim', 'ladki']['I-PER', 'I-PER']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GASvK913tAmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1594de67-da9e-45d7-f5ac-5c4f9e37a432"
      },
      "source": [
        "#给的例子\n",
        "print(len(train_data))\n",
        "print(train_data[1])\n",
        "print(target_y_train[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n",
            "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
            "['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBQkEN8M3GbG"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cCqGvJchOdR"
      },
      "source": [
        "#### Generate word_to_ix and tag_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MWAK0NV3INF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3ae1fda-56be-4e61-94b4-f292586b5942"
      },
      "source": [
        "word_to_ix = {}     #进行了一波预处理\n",
        "for sentence in train_data+validation_data+test_data:\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "word_list = list(word_to_ix.keys())   #2351 token对应inde; index .keys 把关键的元素取出来。就是不要index;按顺序取的感觉\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}     #如下 ；还是BIO\n",
        "for tags in target_y_train+target_y_validation:     #['O', 'O', 'O', 'I-ORG']\n",
        "    for tag in tags:      #每一个的词的tag\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix) #就是又匹配上重新复制了。\n",
        "\n",
        "'''\n",
        "{'<START>': 0,\n",
        " '<STOP>': 1,\n",
        " 'B-LOC': 7,\n",
        " 'B-MISC': 8,\n",
        " 'I-LOC': 6,\n",
        " 'I-MISC': 4,\n",
        " 'I-ORG': 3,\n",
        " 'I-PER': 5,\n",
        " 'O': 2}\n",
        " '''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n{'<START>': 0,\\n '<STOP>': 1,\\n 'B-LOC': 7,\\n 'B-MISC': 8,\\n 'I-LOC': 6,\\n 'I-MISC': 4,\\n 'I-ORG': 3,\\n 'I-PER': 5,\\n 'O': 2}\\n \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEswz2QjhXBM"
      },
      "source": [
        "#### Generate Embedding Matrix 把word变成word-embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oz6KVyjsxM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d23320b-cdc6-4f74-b2b5-fa36bf71e1c3"
      },
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np   #要加的\n",
        "word_emb_model = api.load(\"glove-twitter-25\") \n",
        "\n",
        "EMBEDDING_DIM = 25      #补充到25了\n",
        "\n",
        "embedding_matrix = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "embedding_matrix = np.array(embedding_matrix) #熟悉的节奏lookup tablel; 2维度\n",
        "embedding_matrix.shape            #(2351, 25)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2351, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlchZJO8hdXa"
      },
      "source": [
        "#### convert dataset into idxs   将数据集转换为idxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRs6mouFwEx4"
      },
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:         #sent 每句话\n",
        "        input_index_list.append([to_ix[w] for w in sent])       #w是每个词    查询：【】词加'' word_to_ix['near'] 返回的是每个词的ind\n",
        "    return input_index_list         # 把每个词的inde，又丢进list，一句话都是由inde组成，把这个数据集全都改成了index的模式\n",
        "\n",
        "train_input_index =  to_index(train_data,word_to_ix)    # 句子toke，词字典\n",
        "train_output_index = to_index(target_y_train,tag_to_ix)   #每句没词类别，BIO类别典\n",
        "val_input_index = to_index(validation_data,word_to_ix)    # 句子toke，词字典\n",
        "val_output_index = to_index(target_y_validation,tag_to_ix)    #每句没词类别，BIO类别典\n",
        "test_input_index = to_index(test_data,word_to_ix)   # 句子toke，词字典\n",
        "test_output_index = to_index(target_y_test,tag_to_ix)   #每句没词类别，BIO类别典"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEscWBrhjgb"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIdywNKBjBD1",
        "outputId": "42eb5ef3-5877-4f50-c43f-fa6355f4d4df"
      },
      "source": [
        "'''\n",
        "随机数据\n",
        "\n",
        "为CPU中设置种子，生成随机数\n",
        "torch.manual_seed(number)   用的这\n",
        "为特定GPU设置种子，生成随机数\n",
        "torch.cuda.manual_seed(number)\n",
        "为所有GPU设置种子，生成随机数\n",
        "torch.cuda.manual_seed_all()\n",
        "\n",
        "在需要生成随机数据的实验中，每次实验都需要生成数据。设置随机种子是为了确保每次生成固定的随机数，这就使得每次实验结果显示一致了，有利于实验的比较和改进。\n",
        "'''\n",
        "'''tag_to_ix\n",
        "{'<START>': 0,\n",
        " '<STOP>': 1,\n",
        " 'B-LOC': 7,\n",
        " 'B-MISC': 8,\n",
        " 'I-LOC': 6,\n",
        " 'I-MISC': 4,\n",
        " 'I-ORG': 3,\n",
        " 'I-PER': 5,\n",
        " 'O': 2}\n",
        " '''"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa7afdc3cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHvCh0aymKwK",
        "outputId": "73aa4ec1-f44d-4736-f4ed-47a5df16caec"
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)      #手册"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdd2c8ff5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfWgfSG-lqv6"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)    #2351, 类别的词典， 50 隐藏原。\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu10BzANkjc0"
      },
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):    #vocab_size = 2351；字典里有多少词。类别词典 25最长embedding-长度（每个词）；hidden隐藏元50\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)     #传参了一波\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\"\"#这里我们使用嵌入矩阵作为nn.Embedding的初始权重\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)#\" // \" 表示整数除法\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akKCDhXZkkrm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AgRWakkfmT"
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)      #手册\n",
        "#最大值，最大分数\n",
        "def argmax(vec):      \n",
        "    # return the argmax as a python int# 将argmax作为一个Python int返回\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()     #item() 方法是用来将只有一个元素的numpy数组或tensor张量转化为标量的方法\n",
        "                          #Python 字典 items() 函数以列表返回可遍历的(键, 值) 元组数组。\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm 以数字上稳定的方式计算正向算法的对数和exp \n",
        "#log_sum_exp(next_tag_var).view(1))\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNHEV1kiDKt"
      },
      "source": [
        "#### Function for accuracy [Please Complete this part]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Moqs-zwboIn"
      },
      "source": [
        "Please complete the *cal_acc* function that generates the model predictions using the input data and calculates the accuracy by comparing the model predictions with the ground truth labels. You can refer to the [Train the model] section regarding what the inputs and outputs are and how they will be used.\n",
        "\n",
        "**Hint**: You are going to test the \"model\" using \"input_index\"(whose shape is (num_samples, seq_length)), and you are going to return three variables:\n",
        "- **predicted**: This should be a list, and each item in the list should be the predicted NER label for a word. Suppose you have two sentences, sentence 1 has 8 words and sentence 2 has 6 words. This list should have 14(=8+6) items.\n",
        "- **ground_truth**: This should also be a list, and each item in the list should be the actual NER label for a word, which can be easily gotten from \"output_index\". Suppose you have two sentences, sentence 1 has 8 words and sentence 2 has 6 words. This list should have 14(=8+6) items.\n",
        "- **accuracy**: You are going to use \"predicted\" and \"ground_truth\" to calculate the accuracy.\n",
        "\n",
        "\n",
        "请完成cal_acc函数，该函数使用输入数据生成模型预测，并通过比较模型预测和地面真实标签来计算准确性。关于输入和输出是什么以及如何使用它们，你可以参考[训练模型]部分。\n",
        "\n",
        "提示：你将使用 \"input_index\"（其形状为（num_samples，seq_length））来测试 \"模型\"，你将返回三个变量。\n",
        "\n",
        "预测的。这应该是一个列表，列表中的每一项都应该是一个词的预测的NER标签。假设你有两个句子，句子1有8个词，句子2有6个词。这个列表应该有14（=8+6）项。\n",
        "ground_truth：这也应该是一个列表，列表中的每一项都应该是一个词的实际NER标签，这可以很容易从 \"output_index \"中得到。假设你有两个句子，句子1有8个词，句子2有6个词。这个列表应该有14（=8+6）项。\n",
        "准确性：你要用 \"预测 \"和 \"ground_truth \"来计算准确性。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6byPUkI5hEL"
      },
      "source": [
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "\n",
        " \n",
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    print(model(precheck_sent))\n",
        "# We got it\n",
        "#https://blog.csdn.net/kyle1314608/article/details/100589398?ops_request_misc=&request_id=&biz_id=102&utm_term=python%20Bi-LSTM%20with%20CRF&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-8-.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJmu0oSsjLBm"
      },
      "source": [
        "import numpy as np\n",
        "def cal_acc(model, input_index, output_index):\n",
        "\n",
        "    return predicted, ground_truth, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_82UaXEOhoQQ"
      },
      "source": [
        "Initialising the model 初始化模式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuzZ_et6FD7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UiokVOjPUn"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0CMFVSwlLru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b0ca2e-f6b3-4238-c8fb-1805ecb90885"
      },
      "source": [
        "\"\"\"Each epoch will take about 1-2 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(20):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    # Call the cal_acc functions you implemented as required\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "# The log below is the sample output for this section"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 3404.31, train acc: 0.8501, val loss: 437.62, val acc: 0.8032, time: 47.32s\n",
            "Epoch:2, Training loss: 2076.04, train acc: 0.8808, val loss: 425.70, val acc: 0.8157, time: 47.92s\n",
            "Epoch:3, Training loss: 1580.79, train acc: 0.9072, val loss: 442.27, val acc: 0.8244, time: 47.64s\n",
            "Epoch:4, Training loss: 1328.45, train acc: 0.9104, val loss: 469.88, val acc: 0.8232, time: 47.37s\n",
            "Epoch:5, Training loss: 1150.15, train acc: 0.9247, val loss: 480.14, val acc: 0.8281, time: 48.19s\n",
            "Epoch:6, Training loss: 1018.71, train acc: 0.9315, val loss: 482.81, val acc: 0.8281, time: 47.43s\n",
            "Epoch:7, Training loss: 892.29, train acc: 0.9363, val loss: 506.21, val acc: 0.8281, time: 47.94s\n",
            "Epoch:8, Training loss: 791.39, train acc: 0.9434, val loss: 520.18, val acc: 0.8219, time: 47.44s\n",
            "Epoch:9, Training loss: 679.52, train acc: 0.9499, val loss: 517.10, val acc: 0.8381, time: 47.88s\n",
            "Epoch:10, Training loss: 599.38, train acc: 0.9578, val loss: 540.84, val acc: 0.8356, time: 47.33s\n",
            "Epoch:11, Training loss: 534.61, train acc: 0.9592, val loss: 577.32, val acc: 0.8294, time: 47.75s\n",
            "Epoch:12, Training loss: 463.26, train acc: 0.9646, val loss: 570.23, val acc: 0.8331, time: 47.12s\n",
            "Epoch:13, Training loss: 412.58, train acc: 0.9714, val loss: 588.15, val acc: 0.8306, time: 47.27s\n",
            "Epoch:14, Training loss: 358.13, train acc: 0.9774, val loss: 588.83, val acc: 0.8418, time: 47.37s\n",
            "Epoch:15, Training loss: 324.41, train acc: 0.9822, val loss: 589.63, val acc: 0.8443, time: 47.64s\n",
            "Epoch:16, Training loss: 299.56, train acc: 0.9823, val loss: 587.16, val acc: 0.8518, time: 48.07s\n",
            "Epoch:17, Training loss: 259.59, train acc: 0.9840, val loss: 580.65, val acc: 0.8468, time: 47.72s\n",
            "Epoch:18, Training loss: 233.16, train acc: 0.9810, val loss: 603.09, val acc: 0.8443, time: 48.05s\n",
            "Epoch:19, Training loss: 206.70, train acc: 0.9873, val loss: 600.08, val acc: 0.8568, time: 47.49s\n",
            "Epoch:20, Training loss: 176.60, train acc: 0.9879, val loss: 615.01, val acc: 0.8493, time: 47.35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7ANbv_jSzI"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIh-mWrvi6Pw"
      },
      "source": [
        "# Call the cal_acc functions you implemented as required\n",
        "y_pred, y_true, _ = cal_acc(model,test_input_index,test_output_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAnJVsyPq5kR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f12a8de-11d4-448d-f7e8-df52ef4cb14c"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n",
        "# The log below is the sample output for this section"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       I-LOC     0.8750    0.9333    0.9032        45\n",
            "      I-MISC     0.8800    0.8800    0.8800        25\n",
            "       I-ORG     0.0000    0.0000    0.0000         3\n",
            "       I-PER     0.9135    0.7540    0.8261       126\n",
            "           O     0.9635    0.9839    0.9736       806\n",
            "\n",
            "    accuracy                         0.9473      1005\n",
            "   macro avg     0.7264    0.7102    0.7166      1005\n",
            "weighted avg     0.9483    0.9473    0.9467      1005\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}