{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E8.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMf/uLExXZu0vJ5J41qdFYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/E8_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kg_YnAymt4i"
      },
      "source": [
        "# Exercise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUE5KRTOP9Om"
      },
      "source": [
        "## E1. Please describe two alternative solutions in order to prevent the zero count issue in n-gram language models. Please do not list them up but describe how they work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIJYak3sDH06"
      },
      "source": [
        "E1. è¯·æè¿°ä¸¤ä¸ªå¤‡é€‰çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥é˜²æ­¢n-gramè¯­è¨€æ¨¡å‹ä¸­çš„é›¶è®¡æ•°é—®é¢˜ã€‚è¯·ä¸è¦æŠŠå®ƒä»¬åˆ—å‡ºæ¥ï¼Œè€Œæ˜¯æè¿°å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚\n",
        "[ä¸¤ä¸ªè§£å†³n-gram æ¨¡å‹ä¸­é›¶è®¡æ•°é—®é¢˜ã€‚ç„¶åæè¿°ä»–ä»¬çš„å·¥ä½œæµç¨‹ã€‘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2_rcPymQANX"
      },
      "source": [
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3612UH2DSNB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRivr820ULSk"
      },
      "source": [
        "## E2. Neural Language Model\n",
        "\n",
        "You are required to modify the below example code that can be working with beam search (k > 1)\n",
        "\n",
        " ç¥ç»è¯­è¨€æ¨¡å‹\n",
        "ä½ éœ€è¦ä¿®æ”¹ä¸‹é¢çš„ç¤ºä¾‹ä»£ç ï¼Œå¯ä»¥ç”¨æ³¢æŸæœç´¢ï¼ˆk > 1ï¼‰å·¥ä½œã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKTEsdU_ULSm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egk2F3luQI5g"
      },
      "source": [
        "Now, let's see how to build a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. The objective of this model is to generate new text, given that some input text is present. Lets start building the architecture.\n",
        "\n",
        "ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡å®ç°å’Œè®­ç»ƒæœ€å…ˆè¿›çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¥å»ºç«‹ä¸€ä¸ªç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬çš„è¯­è¨€æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹çš„ç›®æ ‡æ˜¯åœ¨æœ‰ä¸€äº›è¾“å…¥æ–‡æœ¬çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚è®©æˆ‘ä»¬å¼€å§‹å»ºç«‹è¿™ä¸ªæ¶æ„ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDscXKH-C5uv"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIkyodwQO6d"
      },
      "source": [
        "Lets use a popular nursery rhymeâ€Šâ€”â€Šâ€œCat and Her Kittensâ€ as our corpus. A corpus is defined as the collection of text documents.\n",
        "\n",
        "è®©æˆ‘ä»¬ç”¨ä¸€é¦–æµè¡Œçš„ç«¥è°£--ã€ŠçŒ«å’Œå¥¹çš„å°çŒ«ã€‹ä½œä¸ºæˆ‘ä»¬çš„è¯­æ–™åº“ã€‚è¯­æ–™åº“è¢«å®šä¹‰ä¸ºæ–‡æœ¬æ–‡ä»¶çš„é›†åˆã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy-wko0TdkP"
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length   å¡«å……åºåˆ—è‡³æœ€å¤§é•¿åº¦\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data  å‡†å¤‡æ•°æ®\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens  ä½¿ç”¨æ ‡è®°åˆ—è¡¨åˆ›å»ºè¾“å…¥åºåˆ—\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences è¡¥å……åºåˆ— \n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label  åˆ›å»ºé¢„æµ‹å™¨å’Œæ ‡ç­¾\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTSn9to5DxW0"
      },
      "source": [
        "# import torch\n",
        "# torch.cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCDUDPold5T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9425a2c-0103-4c7b-ca74-15fea72b9bd4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)  \n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])\n",
        "        # The reason we are using log_softmax here is that we want to calculate -log(p) and find the minimum score      \n",
        "        #æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨log_softmaxçš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬æƒ³è®¡ç®—-log(p)ï¼Œå¹¶æ‰¾åˆ°æœ€å°åˆ†å€¼ã€‚                                 \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)      \n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda() #10,150,100,43 è¿™ä¸œè¥¿colabæ²¡å¼€GPUè¿˜ç”¨ä¸äº†â€¦â€¦\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):  \n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()       \n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, training loss: 3.6628, training acc: 6.25%\n",
            "Epoch: 20, training loss: 3.4630, training acc: 12.50%\n",
            "Epoch: 30, training loss: 2.9925, training acc: 25.00%\n",
            "Epoch: 40, training loss: 2.4894, training acc: 33.33%\n",
            "Epoch: 50, training loss: 2.1010, training acc: 50.00%\n",
            "Epoch: 60, training loss: 1.7946, training acc: 60.42%\n",
            "Epoch: 70, training loss: 1.5182, training acc: 77.08%\n",
            "Epoch: 80, training loss: 1.2937, training acc: 85.42%\n",
            "Epoch: 90, training loss: 1.1197, training acc: 89.58%\n",
            "Epoch: 100, training loss: 0.9623, training acc: 91.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_B1EX4QRPS"
      },
      "source": [
        "The code below only works with k=1, it does not store the candidates. You need to modify the code to make it work with k > 1.\n",
        "\n",
        "ä¸‹é¢çš„ä»£ç åªåœ¨k=1çš„æƒ…å†µä¸‹å·¥ä½œï¼Œå®ƒä¸å­˜å‚¨å€™é€‰äººã€‚ä½ éœ€è¦ä¿®æ”¹ä»£ç ä»¥ä½¿å…¶åœ¨k>1çš„æƒ…å†µä¸‹å·¥ä½œã€‚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_e6C4pNSP5f"
      },
      "source": [
        "'''\n",
        "å‡è®¾kï¼ˆbeam size) = 2\n",
        "ç„¶åå–å‰k words ç„¶åç®—åˆ†æ•°ã€‚ç„¶åä¸‹ä¸ªè¯çš„å‰kä¸ªè¯è®¡ç®—åˆ†æ•°ã€‚\n",
        "åœ¨è¿™äº›ğ’Œæ–¹å‡è®¾ä¸­ï¼Œåªä¿ç•™å¾—åˆ†æœ€é«˜çš„kï¼›é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å‡è¯´ !\n",
        "æ„Ÿè§‰æ˜¯å…‰æ³¢æ˜¯å‡ å°±å–å‡ ä¸ªæœ€å¤§å€¼ï¼ˆæ€»å…±çš„æœ€å¤§å€¼ï¼‰\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-kB20hGYpq3"
      },
      "source": [
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "from numpy import array\n",
        "from numpy import log\n",
        "data1 = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data1 = array(data1)\n",
        "for step,row in enumerate(data1):  #ç¬¬å‡ æ¬¡ï¼Œç„¶åç¬¬å‡ è¡Œ 0-9æ¬¡ æ¯æ¬¡çš„æ¯è¡Œ\n",
        "  # print(step)\n",
        "   print('\\n')\n",
        "  # # print(row)\n",
        "  #  print(row[0])  # ä¸€ä¸ªå°æ•°\n",
        "  #  print((-log(row[0])))  #æ¯è¡Œçš„ç¬¬å‡ ä¸ª çš„logå€¼\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcU4n714Ywnj",
        "outputId": "f37b98f5-e205-4355-f3ad-5442fb2af5d3"
      },
      "source": [
        "data1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnuqsx5-UbtC",
        "outputId": "6de3888a-c400-48af-8dd5-4bc43321e139"
      },
      "source": [
        "seed_text = \"we naughty\"\n",
        "next_words = 3\n",
        "max_sequence_len = max_sequence_len\n",
        "k = 3\n",
        "\n",
        "seed_candidates = [(seed_text, .0)] \n",
        "seed_candidates\n",
        "for _ in range(next_words): #3  _ = 0,1,2\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb2GwSgUYAlH",
        "outputId": "f79c0bbd-c8de-4bef-b66f-0fa566af4de5"
      },
      "source": [
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)\n",
        "token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "token_list"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24, 41]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oeo5Iuvgbf05",
        "outputId": "15bbb8b0-ee24-4d26-df98-ced6c98d83dc"
      },
      "source": [
        "predicted[0]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-8.782061 , -4.1352005, -4.6741896, -5.423085 , -5.80719  ,\n",
              "       -4.605222 , -2.89474  , -3.1184852, -2.7207785, -4.324253 ,\n",
              "       -4.723193 , -3.7543466, -4.6389623, -6.77596  , -8.168003 ,\n",
              "       -4.536055 , -5.4721437, -2.0728915, -4.5041804, -5.0523615,\n",
              "       -7.471605 , -9.329521 , -3.158932 , -2.082461 , -2.3154752,\n",
              "       -5.262993 , -7.033446 , -3.225051 , -1.5962896, -7.6390343,\n",
              "       -9.032632 , -3.4792678, -6.290392 , -9.204702 , -5.7502337,\n",
              "       -6.5526757, -7.78609  , -4.044515 , -8.748812 , -9.220048 ,\n",
              "       -5.239812 , -7.9917526, -8.861893 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXf8jL7aIgd",
        "outputId": "e18e3174-fda2-4246-a6b2-933aa200b9d7"
      },
      "source": [
        "np.argsort(predicted[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21, 39, 33, 30, 42,  0, 38, 14, 41, 36, 29, 20, 26, 13, 35, 32,  4,\n",
              "       34, 16,  3, 25, 40, 19, 10,  2, 12,  5, 15, 18,  9,  1, 37, 11, 31,\n",
              "       27, 22,  7,  6,  8, 24, 23, 17, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "jmVhJyWIZexq",
        "outputId": "305a8114-2e29-4df2-c70c-c6eec1639482"
      },
      "source": [
        "def get_topK(predicted, k=1):  #é‚£è¿™é‡Œéœ€è¦ä¼ é€’ä¸€ä¸ªk\n",
        "    \n",
        "    # Get the index of the highest k index # è·å¾—æœ€é«˜çš„kæŒ‡æ•°çš„ç´¢å¼•\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # ç”±äºè¾“å…¥çš„åªæ˜¯ä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨[0]æ¥æå–é¢„æµ‹ç»“æœ\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # è¿”å›ä¸€ä¸ªå…ƒç»„çš„åˆ—è¡¨\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "id, s = get_topK(predicted, k)[0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-9ad0f6682966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_topK' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "jMp6X3VAQXR3",
        "outputId": "46e3c3a4-5e2a-4a48-81e9-4ac8549b1b13"
      },
      "source": [
        "seed_text = \"we naughty\"\n",
        "next_words = 3\n",
        "max_sequence_len = max_sequence_len\n",
        "k = 3\n",
        "#ç›®å‰æ˜¯åˆå§‹åŒ–ï¼Œ[('we naughty', 0.0)]\n",
        "seed_candidates = [(seed_text, .0)] #\"we naughty\"  [('we naughty they our kittens', 8.618810653686523)]æŠŠé‡Œé¢çš„æ•°æ®å–å‡ºæ¥\n",
        "for _ in range(next_words): #3 #3  _ = 0,1,2\n",
        "    successives = [] #é€æ¬¡å¯¼æ•°ï¼Ÿ\n",
        "    # if k = 1, len(seed_candidates) will always be 1 # å¦‚æœk = 1ï¼Œlen(seed_candidates)å°†æ€»æ˜¯1\n",
        "    for i in range(len(seed_candidates)): #len(sequences) ç›®å‰é‡Œé¢ä¸€å¥è¯1 è¿™é‡Œè¦æ”¹ã€‚\n",
        "        seed_text, score = seed_candidates[i]  #ä¹‹å‰æ˜¯æ–‡æœ¬å’Œåˆ†æ•°ã€‚åˆçº§ç„¶åæ…¢æ…¢è¿­ä»£æ›´æ–°ã€‚\n",
        "     \n",
        "        token_list = [word_to_index[word] for word in seed_text.split()]  #[24, 41] ç›®å‰åªæœ‰ä¸¤ä¸ªè¯ã€‚æ‰€ä»¥åˆ†å¼€æ‰¾we': 24,'naughty': 41,ï¼Œ å˜æˆindex\n",
        "        token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)   #7æ‰©å……åˆ°æœ€å¤§é•¿åº¦ list\n",
        "\n",
        "        seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)  #ç„¶åå˜æˆtensor é•¿åº¦è¿˜æ˜¯7 é‡Œé¢è¿˜æ˜¯é‚£ä¸¤ä¸ªå•è¯ã€‚æ’åœ¨å€’æ•°çš„ä½ç½®\n",
        "        predicted = model(seed_input).cpu().detach().numpy()  #è·‘äº†é‚£ä¸ªæ¨¡å‹RNNå˜arrayäº† å˜æˆï¼ˆ1ï¼Œ43ï¼‰ è¿™ä¸ªå‘é‡ç»´åº¦æ˜¯43ï¼Œç°æœ‰å¥å­ã€‚\n",
        "\n",
        "        # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "          # å› ä¸ºå®ƒåªå¯¹k = 1èµ·ä½œç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç”¨[0]æ¥è·å¾—å•è¯idå’Œlog(p)\n",
        "        # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "          # ç„¶è€Œï¼Œå¦‚æœk = 3ï¼Œå°±ä¸èƒ½ç®€å•åœ°ç”¨[0]æ¥è·å¾—å€™é€‰è€…äº†\n",
        "        id, s = get_topK(predicted, k)[0]  #æŠŠè¿™å¥è¯çš„arrayéƒ½ä¸¢è¿›å»åšgettopKäº†\n",
        "        # get the output word #è·å¾—è¾“å‡ºå­—æ•°\n",
        "        output_word = ind_to_word(id)\n",
        "        # put the word into the sentence input#æŠŠè¿™ä¸ªè¯æ”¾åˆ°å¥å­çš„è¾“å…¥ä¸­\n",
        "        # calcualte the accumulated score by -log(p)#ç”¨-log(p)è®¡ç®—ç´¯è®¡å¾—åˆ†ã€‚ æ¯è¡Œçš„ç¬¬å‡ ä¸ª çš„logå€¼ \n",
        "        successives.append((seed_text + ' ' + output_word, score - s))  #candidate = [seq + [j], score + (-log(row[j])) ]ã€ã€all_candidates.append(candidate)\n",
        "  #è¿™åé¢ä¸€æ · #æ¯è¡Œçš„ç¬¬å‡ ä¸ª çš„logå€¼\n",
        "    # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "    ## è·å¾—æœ€ä½çš„kä¸ªç´¯ç§¯åˆ†æ•°ï¼ˆæœ€é«˜çš„kä¸ªç´¯ç§¯æ¦‚ç‡ï¼‰ã€‚\n",
        "    # Then, make them as the seed_candidate for the next word to predict\n",
        "    # ç„¶åï¼ŒæŠŠå®ƒä»¬ä½œä¸ºä¸‹ä¸€ä¸ªè¦é¢„æµ‹çš„è¯çš„ç§å­_å€™é€‰è€…\n",
        "    ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "    seed_candidates = ordered[:k]\n",
        "print(seed_candidates[0][0])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-aec70b8cfb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# However, if k = 3, you can't simply use [0] to get the candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;31m# ç„¶è€Œï¼Œå¦‚æœk = 3ï¼Œå°±ä¸èƒ½ç®€å•åœ°ç”¨[0]æ¥è·å¾—å€™é€‰è€…äº†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# get the output word #è·å¾—è¾“å‡ºå­—æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_topK' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMpc8OM1RCVC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXqqClKiQH5D"
      },
      "source": [
        "top_k = np.argsort(predicted[0])[-k:]   #array([32, 10, 29]) è®¡ç®—æ•°ç»„æ’åºçš„ä¸‹æ ‡;\n",
        "predicted "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpVTH-FPohU"
      },
      "source": [
        "predicted_ind = id\n",
        "for word, index in word_to_index.items():\n",
        "    if index == predicted_ind:\n",
        "      print(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EcDDk2eRmzu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J41-4T9NRjrI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB4b1Zj2PPEh"
      },
      "source": [
        "# convert index to word  å°†ç´¢å¼•è½¬æ¢ä¸ºå•è¯ ç´¢å¼•ç­‰äºé¢„æµ‹çš„å°±è¿”å›å•è¯\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results è·å¾—å‰kä¸ªæœ€æœ‰é¢„æµ‹æ€§çš„ç»“æœ\n",
        "def get_topK(predicted, k=1):  #é‚£è¿™é‡Œéœ€è¦ä¼ é€’ä¸€ä¸ªk\n",
        "    \n",
        "    # Get the index of the highest k index # è·å¾—æœ€é«˜çš„kæŒ‡æ•°çš„ç´¢å¼•\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # ç”±äºè¾“å…¥çš„åªæ˜¯ä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨[0]æ¥æå–é¢„æµ‹ç»“æœ\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # è¿”å›ä¸€ä¸ªå…ƒç»„çš„åˆ—è¡¨\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "# To-Do: modify this function# å¾…åŠäº‹é¡¹ï¼šä¿®æ”¹æ­¤åŠŸèƒ½\n",
        "# Generate text, currently only works with k=1 # ç”Ÿæˆæ–‡æœ¬ï¼Œç›®å‰åªåœ¨k=1æ—¶æœ‰æ•ˆ \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "## æç¤ºï¼šæœ€ç®€å•çš„æ–¹æ³•æ˜¯ä¿®æ”¹ç¬¬40-47è¡Œçš„ä»£ç ï¼Œä½†è¿™å¹¶ä¸æ˜¯å¼ºåˆ¶æ€§çš„\n",
        "# beam search å…‰æŸæœç´¢"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FcJs69JQWAT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOcBntHzKSAK"
      },
      "source": [
        "# convert index to word  å°†ç´¢å¼•è½¬æ¢ä¸ºå•è¯ ç´¢å¼•ç­‰äºé¢„æµ‹çš„å°±è¿”å›å•è¯\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results è·å¾—å‰kä¸ªæœ€æœ‰é¢„æµ‹æ€§çš„ç»“æœ\n",
        "def get_topK(predicted, k=1):  #é‚£è¿™é‡Œéœ€è¦ä¼ é€’ä¸€ä¸ªk\n",
        "    \n",
        "    # Get the index of the highest k index # è·å¾—æœ€é«˜çš„kæŒ‡æ•°çš„ç´¢å¼•\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # ç”±äºè¾“å…¥çš„åªæ˜¯ä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨[0]æ¥æå–é¢„æµ‹ç»“æœ\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # è¿”å›ä¸€ä¸ªå…ƒç»„çš„åˆ—è¡¨\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "# To-Do: modify this function# å¾…åŠäº‹é¡¹ï¼šä¿®æ”¹æ­¤åŠŸèƒ½\n",
        "# Generate text, currently only works with k=1 # ç”Ÿæˆæ–‡æœ¬ï¼Œç›®å‰åªåœ¨k=1æ—¶æœ‰æ•ˆ \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "## æç¤ºï¼šæœ€ç®€å•çš„æ–¹æ³•æ˜¯ä¿®æ”¹ç¬¬40-47è¡Œçš„ä»£ç ï¼Œä½†è¿™å¹¶ä¸æ˜¯å¼ºåˆ¶æ€§çš„\n",
        "# beam search å…‰æŸæœç´¢\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1 # å¦‚æœk = 1ï¼Œlen(seed_candidates)å°†æ€»æ˜¯1\n",
        "        for i in range(len(seed_candidates)): #len(sequences)\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            for j in range(len(row)):\n",
        "              candidate = [seq + [j], score + (-log(row[j])) ]  #we are summing up the negative log, so we need to find the minimum score(which is the highest prob)\n",
        "              successives.append(candidate)\n",
        " \n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "             # å› ä¸ºå®ƒåªå¯¹k = 1èµ·ä½œç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç”¨[0]æ¥è·å¾—å•è¯idå’Œlog(p)\n",
        "            # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "             # ç„¶è€Œï¼Œå¦‚æœk = 3ï¼Œå°±ä¸èƒ½ç®€å•åœ°ç”¨[0]æ¥è·å¾—å€™é€‰è€…äº†\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word #è·å¾—è¾“å‡ºå­—æ•°\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input#æŠŠè¿™ä¸ªè¯æ”¾åˆ°å¥å­çš„è¾“å…¥ä¸­\n",
        "            # calcualte the accumulated score by -log(p)#ç”¨-log(p)è®¡ç®—ç´¯è®¡å¾—åˆ†ã€‚\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "      #è¿™åé¢ä¸€æ ·\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        ## è·å¾—æœ€ä½çš„kä¸ªç´¯ç§¯åˆ†æ•°ï¼ˆæœ€é«˜çš„kä¸ªç´¯ç§¯æ¦‚ç‡ï¼‰ã€‚\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        # ç„¶åï¼ŒæŠŠå®ƒä»¬ä½œä¸ºä¸‹ä¸€ä¸ªè¦é¢„æµ‹çš„è¯çš„ç§å­_å€™é€‰è€…\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, 1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, 3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n",
        "#åœ¨è§£ç å™¨çš„æ¯ä¸€æ­¥ï¼Œè·Ÿè¸ªkä¸ªæœ€æœ‰å¯èƒ½çš„éƒ¨åˆ†åºåˆ—ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºå‡è®¾ï¼‰--Kæ˜¯æ³¢æŸå¤§å°ï¼ˆåœ¨å®è·µä¸­çº¦ä¸º5è‡³10ï¼‰\n",
        "#é€‰å‡ºå‰kä¸ªè¯å¹¶è®¡ç®—åˆ†æ•°"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyxvrzWDPAcz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJYgwx1PAl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpunEmZNoLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfcbc61-69f7-42a0-ccf3-0e810005cc3d"
      },
      "source": [
        "# convert index to word  å°†ç´¢å¼•è½¬æ¢ä¸ºå•è¯ ç´¢å¼•ç­‰äºé¢„æµ‹çš„å°±è¿”å›å•è¯\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results è·å¾—å‰kä¸ªæœ€æœ‰é¢„æµ‹æ€§çš„ç»“æœ\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index # è·å¾—æœ€é«˜çš„kæŒ‡æ•°çš„ç´¢å¼•\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # ç”±äºè¾“å…¥çš„åªæ˜¯ä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨[0]æ¥æå–é¢„æµ‹ç»“æœ\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # è¿”å›ä¸€ä¸ªå…ƒç»„çš„åˆ—è¡¨\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# To-Do: modify this function# å¾…åŠäº‹é¡¹ï¼šä¿®æ”¹æ­¤åŠŸèƒ½\n",
        "# Generate text, currently only works with k=1 # ç”Ÿæˆæ–‡æœ¬ï¼Œç›®å‰åªåœ¨k=1æ—¶æœ‰æ•ˆ \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "## æç¤ºï¼šæœ€ç®€å•çš„æ–¹æ³•æ˜¯ä¿®æ”¹ç¬¬40-47è¡Œçš„ä»£ç ï¼Œä½†è¿™å¹¶ä¸æ˜¯å¼ºåˆ¶æ€§çš„\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1 # å¦‚æœk = 1ï¼Œlen(seed_candidates)å°†æ€»æ˜¯1\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "             # å› ä¸ºå®ƒåªå¯¹k = 1èµ·ä½œç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç”¨[0]æ¥è·å¾—å•è¯idå’Œlog(p)\n",
        "            # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "             # ç„¶è€Œï¼Œå¦‚æœk = 3ï¼Œå°±ä¸èƒ½ç®€å•åœ°ç”¨[0]æ¥è·å¾—å€™é€‰è€…äº†\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word #è·å¾—è¾“å‡ºå­—æ•°\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input#æŠŠè¿™ä¸ªè¯æ”¾åˆ°å¥å­çš„è¾“å…¥ä¸­\n",
        "            # calcualte the accumulated score by -log(p)#ç”¨-log(p)è®¡ç®—ç´¯è®¡å¾—åˆ†ã€‚\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        ## è·å¾—æœ€ä½çš„kä¸ªç´¯ç§¯åˆ†æ•°ï¼ˆæœ€é«˜çš„kä¸ªç´¯ç§¯æ¦‚ç‡ï¼‰ã€‚\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        # ç„¶åï¼ŒæŠŠå®ƒä»¬ä½œä¸ºä¸‹ä¸€ä¸ªè¦é¢„æµ‹çš„è¯çš„ç§å­_å€™é€‰è€…\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty go to to\n",
            "we naughty her kittens go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYf-gtT-Uiuh"
      },
      "source": [
        "**Sample Output** (Your output would be different, it is based on the trained model)\n",
        "\n",
        "\n",
        "```\n",
        "we naughty lost their mittens\n",
        "```\n",
        "\n"
      ]
    }
  ]
}