{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E8.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhpNJ5j+9UuJ3lE7xK4lS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Ex/blob/main/E8_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kg_YnAymt4i"
      },
      "source": [
        "# Exercise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUE5KRTOP9Om"
      },
      "source": [
        "## E1. Please describe two alternative solutions in order to prevent the zero count issue in n-gram language models. Please do not list them up but describe how they work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIJYak3sDH06"
      },
      "source": [
        "E1. 请描述两个备选的解决方案，以防止n-gram语言模型中的零计数问题。请不要把它们列出来，而是描述它们是如何工作的。\n",
        "[两个解决n-gram 模型中零计数问题。然后描述他们的工作流程】"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2_rcPymQANX"
      },
      "source": [
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3612UH2DSNB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRivr820ULSk"
      },
      "source": [
        "## E2. Neural Language Model\n",
        "\n",
        "You are required to modify the below example code that can be working with beam search (k > 1)\n",
        "\n",
        " 神经语言模型\n",
        "你需要修改下面的示例代码，可以用波束搜索（k > 1）工作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKTEsdU_ULSm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egk2F3luQI5g"
      },
      "source": [
        "Now, let's see how to build a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. The objective of this model is to generate new text, given that some input text is present. Lets start building the architecture.\n",
        "\n",
        "现在，让我们看看如何通过实现和训练最先进的循环神经网络来建立一个生成自然语言文本的语言模型。这个模型的目标是在有一些输入文本的情况下，生成新的文本。让我们开始建立这个架构。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDscXKH-C5uv"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIkyodwQO6d"
      },
      "source": [
        "Lets use a popular nursery rhyme — “Cat and Her Kittens” as our corpus. A corpus is defined as the collection of text documents.\n",
        "\n",
        "让我们用一首流行的童谣--《猫和她的小猫》作为我们的语料库。语料库被定义为文本文件的集合。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy-wko0TdkP"
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length   填充序列至最大长度\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data  准备数据\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens  使用标记列表创建输入序列\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences 补充序列 \n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label  创建预测器和标签\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTSn9to5DxW0"
      },
      "source": [
        "# import torch\n",
        "# torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCDUDPold5T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223c1a20-eefc-41f3-9d3b-230c29fbd605"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)  \n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])\n",
        "        # The reason we are using log_softmax here is that we want to calculate -log(p) and find the minimum score      \n",
        "        #我们在这里使用log_softmax的原因是，我们想计算-log(p)，并找到最小分值。                                 \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)      \n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda() #10,150,100,43 这东西colab没开GPU还用不了……\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):  \n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()       \n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, training loss: 3.6545, training acc: 8.33%\n",
            "Epoch: 20, training loss: 3.4209, training acc: 10.42%\n",
            "Epoch: 30, training loss: 2.9872, training acc: 14.58%\n",
            "Epoch: 40, training loss: 2.5351, training acc: 22.92%\n",
            "Epoch: 50, training loss: 2.1439, training acc: 52.08%\n",
            "Epoch: 60, training loss: 1.8357, training acc: 68.75%\n",
            "Epoch: 70, training loss: 1.5770, training acc: 77.08%\n",
            "Epoch: 80, training loss: 1.4555, training acc: 79.17%\n",
            "Epoch: 90, training loss: 1.2344, training acc: 81.25%\n",
            "Epoch: 100, training loss: 1.0418, training acc: 89.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_B1EX4QRPS"
      },
      "source": [
        "The code below only works with k=1, it does not store the candidates. You need to modify the code to make it work with k > 1.\n",
        "\n",
        "下面的代码只在k=1的情况下工作，它不存储候选人。你需要修改代码以使其在k>1的情况下工作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_e6C4pNSP5f"
      },
      "source": [
        "'''\n",
        "假设k（beam size) = 2\n",
        "然后取前k words 然后算分数。然后下个词的前k个词计算分数。\n",
        "在这些𝒌方假设中，只保留得分最高的k；选择得分最高的假说 !\n",
        "感觉是光波是几就取几个最大值（总共的最大值）\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-kB20hGYpq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7891f6-ae5c-4d2e-a660-121bd4715de5"
      },
      "source": [
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "from numpy import array\n",
        "from numpy import log\n",
        "data1 = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data1 = array(data1)\n",
        "for step,row in enumerate(data1):  #第几次，然后第几行 0-9次 每次的每行\n",
        "  # print(step)\n",
        "   print('\\n')\n",
        "  # # print(row)\n",
        "  #  print(row[0])  # 一个小数\n",
        "  #  print((-log(row[0])))  #每行的第几个 的log值\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcU4n714Ywnj",
        "outputId": "1524e916-8925-442f-a9bc-39ef3c696cee"
      },
      "source": [
        "data1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1],\n",
              "       [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "       [0.5, 0.4, 0.3, 0.2, 0.1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnuqsx5-UbtC",
        "outputId": "6de3888a-c400-48af-8dd5-4bc43321e139"
      },
      "source": [
        "seed_text = \"we naughty\"\n",
        "next_words = 3\n",
        "max_sequence_len = max_sequence_len\n",
        "k = 3\n",
        "\n",
        "seed_candidates = [(seed_text, .0)] \n",
        "seed_candidates\n",
        "for _ in range(next_words): #3  _ = 0,1,2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb2GwSgUYAlH",
        "outputId": "f79c0bbd-c8de-4bef-b66f-0fa566af4de5"
      },
      "source": [
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)\n",
        "token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "token_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24, 41]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oeo5Iuvgbf05",
        "outputId": "70ac2cf7-f967-4927-a3ab-b5ea3079cc17"
      },
      "source": [
        "predicted[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-9.145516 , -8.109267 , -7.940792 , -6.1404257, -1.2899072,\n",
              "       -3.022852 , -4.556643 , -2.445673 , -2.6552176, -5.9843636,\n",
              "       -4.4980793, -7.4669857, -5.933133 , -3.6852694, -6.186511 ,\n",
              "       -7.40804  , -5.855938 , -3.9005868, -7.244426 , -2.4567208,\n",
              "       -8.26527  , -9.467429 , -6.2680283, -4.155842 , -3.0354843,\n",
              "       -6.5130568, -8.289023 , -5.483369 , -1.6300032, -7.070237 ,\n",
              "       -8.9612665, -5.156124 , -5.3352137, -8.622119 , -6.321787 ,\n",
              "       -4.148204 , -3.5803058, -4.0651474, -7.588977 , -7.5636854,\n",
              "       -5.81188  , -5.2577076, -8.663109 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXf8jL7aIgd",
        "outputId": "60136f5b-c462-492c-a929-79b3dc621212"
      },
      "source": [
        "np.argsort(predicted[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21,  0, 30, 42, 33, 26, 20,  1,  2, 38, 39, 11, 15, 18, 29, 25, 34,\n",
              "       22, 14,  3,  9, 12, 16, 40, 27, 32, 41, 31,  6, 10, 23, 35, 37, 17,\n",
              "       13, 36, 24,  5,  8, 19,  7, 28,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh6stVD6XonL",
        "outputId": "be02c3a1-0d7a-496c-d3bb-b6e9a68934a3"
      },
      "source": [
        "np.argsort(predicted[0])[-1:]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoU9WUQ0Y2So",
        "outputId": "6c5c928b-d5d9-47d9-cdf0-74c769d03ec4"
      },
      "source": [
        "np.argsort(predicted[0])[-3:]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 28,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRx-xIsUY55p"
      },
      "source": [
        "top_k = np.argsort(predicted[0])[-1:]\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtz4XDeMZGMM",
        "outputId": "6565eea2-2cbd-47dd-90d8-ede5282e3ff1"
      },
      "source": [
        "N= [(id, predicted[0][id]) for id in top_k]  #\n",
        "N"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, -1.2899072)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNCbHvjRZiNX",
        "outputId": "c99d9b11-ee89-4d28-bea6-f91e4a01b6a7"
      },
      "source": [
        "id, s = get_topK(predicted, k)[0]     #7  -2.445673\n",
        "s"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.445673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmVhJyWIZexq"
      },
      "source": [
        "def get_topK(predicted, k=1):  #那这里需要传递一个k\n",
        "    \n",
        "    # Get the index of the highest k index # 获得最高的k指数的索引\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # 由于输入的只是一个句子，我们可以用[0]来提取预测结果\n",
        "    top_k = np.argsort(predicted[0])[-k:] #返回从小到大拍的位置; 倒着取就是最大的呗。 array\n",
        "\n",
        "    # return a list of tuple  # 返回一个元组的列表\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k] # N = xxx  [(4, -1.2899072)] 合在了一起。把位置和最大的值取出来了。\n",
        "\n",
        "id, s = get_topK(predicted, k)[0]   #所以三个的话，这里要有个for循环。"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WF-is5HZz1V"
      },
      "source": [
        "def get_topK(predicted, k=1):  #那这里需要传递一个k\n",
        "    \n",
        "    # Get the index of the highest k index # 获得最高的k指数的索引\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # 由于输入的只是一个句子，我们可以用[0]来提取预测结果\n",
        "    top_k = np.argsort(predicted[0])[-k:] #返回从小到大拍的位置; 倒着取就是最大的呗。 array\n",
        "\n",
        "    # return a list of tuple  # 返回一个元组的列表\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k] # N = xxx  [(4, -1.2899072)] 合在了一起。把位置和最大的值取出来了。\n",
        "\n",
        "id, s = get_topK(predicted, k)[0]   #所以三个的话，这里要有个for循环。"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a31VYHu5aSM3",
        "outputId": "fef68c40-bee6-4f6e-d9ca-3f48390b6f11"
      },
      "source": [
        "top_k = np.argsort(predicted[0])[-k:]  \n",
        "top_k"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 28,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkRmpVkYa0TE",
        "outputId": "8ce42a74-ea0a-4ed4-eb7d-0c620d409b5f"
      },
      "source": [
        "predicted"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-9.145516 , -8.109267 , -7.940792 , -6.1404257, -1.2899072,\n",
              "        -3.022852 , -4.556643 , -2.445673 , -2.6552176, -5.9843636,\n",
              "        -4.4980793, -7.4669857, -5.933133 , -3.6852694, -6.186511 ,\n",
              "        -7.40804  , -5.855938 , -3.9005868, -7.244426 , -2.4567208,\n",
              "        -8.26527  , -9.467429 , -6.2680283, -4.155842 , -3.0354843,\n",
              "        -6.5130568, -8.289023 , -5.483369 , -1.6300032, -7.070237 ,\n",
              "        -8.9612665, -5.156124 , -5.3352137, -8.622119 , -6.321787 ,\n",
              "        -4.148204 , -3.5803058, -4.0651474, -7.588977 , -7.5636854,\n",
              "        -5.81188  , -5.2577076, -8.663109 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZQ65uyZa2Jc",
        "outputId": "8129cd2e-777f-4ac3-e953-e7e8337885b2"
      },
      "source": [
        "word_to_index.items()\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('the', 0), ('cat', 1), ('and', 2), ('her', 3), ('kittens', 4), ('they', 5), ('put', 6), ('on', 7), ('their', 8), ('mittens', 9), ('to', 10), ('eat', 11), ('a', 12), ('christmas', 13), ('pie', 14), ('poor', 15), ('little', 16), ('lost', 17), ('then', 18), ('began', 19), ('cry', 20), ('o', 21), ('mother', 22), ('dear', 23), ('we', 24), ('sadly', 25), ('fear', 26), ('cannot', 27), ('go', 28), ('day', 29), ('for', 30), ('have', 31), ('our', 32), ('if', 33), ('it', 34), ('be', 35), ('so', 36), ('ye', 37), ('shall', 38), ('not', 39), ('are', 40), ('naughty', 41)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25C736-LZ4An",
        "outputId": "78c0a41a-83e0-4000-fd03-f7c3e07d3790"
      },
      "source": [
        "top_k = np.argsort(predicted[0])[-k:]   #array([32, 10, 29]) 计算数组排序的下标;  目前k=3 array([ 7, 28,  4]))排序后的最大三个位置\n",
        "predicted \n",
        "#ind_to_word\n",
        "predicted_ind = id    #7  这里最少是k个id啊；应该是个list\n",
        "for word, index in word_to_index.items(): #word_to_index.items()变成一对一对的了\n",
        "    if index == predicted_ind:    #然后把这个单词和index找出来，就是把index投进去\n",
        "      print(word)\n",
        "      print(index)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annsW0oIbn1-",
        "outputId": "af78fe72-027a-48d3-ab71-7a5d4243940d"
      },
      "source": [
        "score"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.198000431060791"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4T0LuEQbbVV",
        "outputId": "093853b4-2403-48bb-b03a-c2fcdea27366"
      },
      "source": [
        "successives.append((seed_text + ' ' + output_word, score - s))  \n",
        "successives"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we naughty on their cry', 6.302371978759766),\n",
              " ('we naughty on their cry', 6.302371978759766),\n",
              " ('we naughty on their cry', 6.302371978759766)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mflooHX1bxi8",
        "outputId": "5b5a0de1-f6cc-4876-9799-2e7961440a87"
      },
      "source": [
        "ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "ordered"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we naughty on their cry', 6.302371978759766),\n",
              " ('we naughty on their cry', 6.302371978759766),\n",
              " ('we naughty on their cry', 6.302371978759766)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHWEEGjJb8kw",
        "outputId": "36bda870-dc89-48cb-de32-823e7acdfbe2"
      },
      "source": [
        "seed_candidates = ordered[:3]  #k = 1就是一个\n",
        "seed_candidates\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we naughty on their cry', 6.302371978759766),\n",
              " ('we naughty on their cry', 6.302371978759766),\n",
              " ('we naughty on their cry', 6.302371978759766)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aochxyixcHe3",
        "outputId": "bea90eea-b6ed-4b16-bf3d-8efea61e75aa"
      },
      "source": [
        "print(seed_candidates[0][0])  #只打出来值。不要后面的数"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty on their cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJubaO3ScNyG"
      },
      "source": [
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMp6X3VAQXR3",
        "outputId": "9fd89a6f-6af3-4d31-8c33-5b5ee1550ae6"
      },
      "source": [
        "seed_text = \"we naughty\"\n",
        "next_words = 3\n",
        "max_sequence_len = max_sequence_len\n",
        "#k = 3 这就恒定了\n",
        "#目前是初始化，[('we naughty', 0.0)]\n",
        "seed_candidates = [(seed_text, .0)] #\"we naughty\"  [('we naughty they our kittens', 8.618810653686523)]把里面的数据取出来\n",
        "for _ in range(next_words): #3 #3  _ = 0,1,2\n",
        "    successives = [] #逐次导数？\n",
        "    # if k = 1, len(seed_candidates) will always be 1 # 如果k = 1，len(seed_candidates)将总是1\n",
        "    for i in range(len(seed_candidates)): #len(sequences) 目前里面一句话1 这里要改。\n",
        "        seed_text, score = seed_candidates[i]  #之前是文本和分数。初级然后慢慢迭代更新。\n",
        "     \n",
        "        token_list = [word_to_index[word] for word in seed_text.split()]  #[24, 41] 目前只有两个词。所以分开找we': 24,'naughty': 41,， 变成index\n",
        "        token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)   #7扩充到最大长度 list\n",
        "\n",
        "        seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)  #然后变成tensor 长度还是7 里面还是那两个单词。排在倒数的位置\n",
        "        predicted = model(seed_input).cpu().detach().numpy()  #跑了那个模型RNN变array了 变成（1，43） 这个向量维度是43，现有句子。\n",
        "\n",
        "        # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "          # 因为它只对k = 1起作用，我们可以简单地用[0]来获得单词id和log(p)\n",
        "        # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "          # 然而，如果k = 3，就不能简单地用[0]来获得候选者了\n",
        "        id, s = get_topK(predicted, k)[0]  #把这句话的array都丢进去做gettopK了\n",
        "        # get the output word #获得输出字数\n",
        "        output_word = ind_to_word(id)\n",
        "        # put the word into the sentence input#把这个词放到句子的输入中\n",
        "        # calcualte the accumulated score by -log(p)#用-log(p)计算累计得分。 每行的第几个 的log值 \n",
        "        successives.append((seed_text + ' ' + output_word, score - s))  #candidate = [seq + [j], score + (-log(row[j])) ]、、all_candidates.append(candidate)\n",
        "  #这后面一样 #每行的第几个 的log值  #score最开始的得分\n",
        "    # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "    ## 获得最低的k个累积分数（最高的k个累积概率）。\n",
        "    # Then, make them as the seed_candidate for the next word to predict\n",
        "    # 然后，把它们作为下一个要预测的词的种子_候选者\n",
        "    ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "    seed_candidates = ordered[:k]\n",
        "print(seed_candidates[0][0])\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty on their cry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jYOfLMgb7yD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMpc8OM1RCVC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXqqClKiQH5D"
      },
      "source": [
        "top_k = np.argsort(predicted[0])[-k:]   #array([32, 10, 29]) 计算数组排序的下标;\n",
        "predicted "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJpVTH-FPohU"
      },
      "source": [
        "predicted_ind = id\n",
        "for word, index in word_to_index.items():\n",
        "    if index == predicted_ind:\n",
        "      print(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EcDDk2eRmzu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J41-4T9NRjrI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB4b1Zj2PPEh"
      },
      "source": [
        "# convert index to word  将索引转换为单词 索引等于预测的就返回单词\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results 获得前k个最有预测性的结果\n",
        "def get_topK(predicted, k=1):  #那这里需要传递一个k\n",
        "    \n",
        "    # Get the index of the highest k index # 获得最高的k指数的索引\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # 由于输入的只是一个句子，我们可以用[0]来提取预测结果\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # 返回一个元组的列表\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "# To-Do: modify this function# 待办事项：修改此功能\n",
        "# Generate text, currently only works with k=1 # 生成文本，目前只在k=1时有效 \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "## 提示：最简单的方法是修改第40-47行的代码，但这并不是强制性的\n",
        "# beam search 光束搜索"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FcJs69JQWAT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOcBntHzKSAK"
      },
      "source": [
        "# convert index to word  将索引转换为单词 索引等于预测的就返回单词\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results 获得前k个最有预测性的结果\n",
        "def get_topK(predicted, k=1):  #那这里需要传递一个k\n",
        "    \n",
        "    # Get the index of the highest k index # 获得最高的k指数的索引\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # 由于输入的只是一个句子，我们可以用[0]来提取预测结果\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # 返回一个元组的列表\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "# To-Do: modify this function# 待办事项：修改此功能\n",
        "# Generate text, currently only works with k=1 # 生成文本，目前只在k=1时有效 \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "## 提示：最简单的方法是修改第40-47行的代码，但这并不是强制性的\n",
        "# beam search 光束搜索\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1 # 如果k = 1，len(seed_candidates)将总是1\n",
        "        for i in range(len(seed_candidates)): #len(sequences)\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            for j in range(len(row)):\n",
        "              candidate = [seq + [j], score + (-log(row[j])) ]  #we are summing up the negative log, so we need to find the minimum score(which is the highest prob)\n",
        "              successives.append(candidate)\n",
        " \n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "             # 因为它只对k = 1起作用，我们可以简单地用[0]来获得单词id和log(p)\n",
        "            # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "             # 然而，如果k = 3，就不能简单地用[0]来获得候选者了\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word #获得输出字数\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input#把这个词放到句子的输入中\n",
        "            # calcualte the accumulated score by -log(p)#用-log(p)计算累计得分。\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "      #这后面一样\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        ## 获得最低的k个累积分数（最高的k个累积概率）。\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        # 然后，把它们作为下一个要预测的词的种子_候选者\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, 1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, 3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n",
        "#在解码器的每一步，跟踪k个最有可能的部分序列（我们称之为假设）--K是波束大小（在实践中约为5至10）\n",
        "#选出前k个词并计算分数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyxvrzWDPAcz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJYgwx1PAl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpunEmZNoLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfcbc61-69f7-42a0-ccf3-0e810005cc3d"
      },
      "source": [
        "# convert index to word  将索引转换为单词 索引等于预测的就返回单词\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results 获得前k个最有预测性的结果\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index # 获得最高的k指数的索引\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    # # 由于输入的只是一个句子，我们可以用[0]来提取预测结果\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple  # 返回一个元组的列表\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# To-Do: modify this function# 待办事项：修改此功能\n",
        "# Generate text, currently only works with k=1 # 生成文本，目前只在k=1时有效 \n",
        "# Hint: The easist way is modifying the code from line 40-47, but it is not compulsory\n",
        "## 提示：最简单的方法是修改第40-47行的代码，但这并不是强制性的\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1 # 如果k = 1，len(seed_candidates)将总是1\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "             # 因为它只对k = 1起作用，我们可以简单地用[0]来获得单词id和log(p)\n",
        "            # However, if k = 3, you can't simply use [0] to get the candidates\n",
        "             # 然而，如果k = 3，就不能简单地用[0]来获得候选者了\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word #获得输出字数\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input#把这个词放到句子的输入中\n",
        "            # calcualte the accumulated score by -log(p)#用-log(p)计算累计得分。\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        ## 获得最低的k个累积分数（最高的k个累积概率）。\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        # 然后，把它们作为下一个要预测的词的种子_候选者\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we naughty go to to\n",
            "we naughty her kittens go\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYf-gtT-Uiuh"
      },
      "source": [
        "**Sample Output** (Your output would be different, it is based on the trained model)\n",
        "\n",
        "\n",
        "```\n",
        "we naughty lost their mittens\n",
        "```\n",
        "\n"
      ]
    }
  ]
}